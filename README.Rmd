---
output: github_document
---

```{r, include = FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(1)
options(
    datatable.print.nrows = 10,
    datatable.print.class = FALSE,
    datatable.print.keys = FALSE,
    width = 200)
```

# mlr3tuning

Package website: [release](https://mlr3tuning.mlr-org.com/) | [dev](https://mlr3tuning.mlr-org.com/dev/)

<!-- badges: start -->
[![tic](https://github.com/mlr-org/mlr3tuning/workflows/tic/badge.svg?branch=main)](https://github.com/mlr-org/mlr3tuning/actions)
[![CRAN Status](https://www.r-pkg.org/badges/version-ago/mlr3tuning)](https://cran.r-project.org/package=mlr3tuning)
[![StackOverflow](https://img.shields.io/badge/stackoverflow-mlr3-orange.svg)](https://stackoverflow.com/questions/tagged/mlr3)
[![Mattermost](https://img.shields.io/badge/chat-mattermost-orange.svg)](https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/)
<!-- badges: end -->

*mlr3tuning* is the hyperparameter optimization package of the [mlr3](https://mlr-org.com/) ecosystem.
It features highly configurable search spaces via the [paradox](https://github.com/mlr-org/paradox) package and finds optimal hyperparameter configurations for any mlr3 [learner](https://github.com/mlr-org/mlr3learners).
mlr3tuning works with several optimization algorithms e.g. Random Search, Iterated Racing, Bayesian Optimization (in [mlr3mbo](https://github.com/mlr-org/mlr3mbo)) and Hyperband (in [mlr3hyperband](https://github.com/mlr-org/mlr3hyperband)).
Moreover, it can [automatically](https://mlr3book.mlr-org.com/optimization.html#sec-autotuner) optimize learners and estimate the performance of optimized models with [nested resampling](https://mlr3book.mlr-org.com/optimization.html#sec-nested-resampling).
The package is built on the optimization framework [bbotk](https://github.com/mlr-org/bbotk).

## Extension packages

mlr3tuning is extended by the following packages.

* [mlr3tuningspaces](https://github.com/mlr-org/mlr3tuningspaces) is a collection of search spaces from scientific articles for commonly used learners.
* [mlr3hyperband](https://github.com/mlr-org/mlr3hyperband) adds the Hyperband and Successive Halving algorithm.
* [mlr3mbo](https://github.com/mlr-org/mlr3mbo) adds Bayesian Optimization methods.

## Resources

There are several sections about hyperparameter optimization in the [mlr3book](https://mlr3book.mlr-org.com).

* Getting started with [Hyperparameter Optimization](https://mlr3book.mlr-org.com/optimization.html)
* [Tune](https://mlr3book.mlr-org.com/optimization.html#sec-tuning-instance) a simple classification tree on the Palmer Penguins data set.
* Learn about [Tuning Spaces](https://mlr3book.mlr-org.com/technical.html#sec-tuning-space).
* Estimate Model Performance with [Nested Resampling](https://mlr3book.mlr-org.com/optimization.html#sec-model-performance).

The [gallery](https://mlr-org.com/gallery.html#category:tuning) features a collection of case studies and demos about optimization.

* [Practical Tuning Series](https://mlr-org.com/gallery.html#category:practical_tuning_series)
* [Tuning Search Spaces](https://mlr3book.mlr-org.com/optimization.html#searchspace)
* [Nested Resampling](https://mlr3book.mlr-org.com/optimization.html#nested-resampling)

The [cheatsheet](https://cheatsheets.mlr-org.com/mlr3tuning.pdf) summarizes the most important functions of mlr3tuning.

## Installation

Install the last release from CRAN:

```{r eval = FALSE}
install.packages("mlr3tuning")
```

Install the development version from GitHub:

```{r eval = FALSE}
remotes::install_github("mlr-org/mlr3tuning")
```

## Examples

```{r, include = FALSE}
# mute load messages
library("mlr3tuning")
```

### Basic Hyperparameter Optimization

```{r}
library("mlr3tuning")

# load learner and set search space
learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE))

# hyperparameter tuning on the Pima Indians Diabetes data set
instance = tune(
  method = tnr("random_search"),
  task = tsk("pima"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10,
  batch_size = 5
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)[, list(batch_nr, cp, classif.ce, resample_result)]

# fit the final model on the complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("pima"))
```

### Automatic Tuning

```{r}
# construct auto tuner
at = auto_tuner(
  method = tnr("random_search"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10,
  batch_size = 5
)

# train/test split
task = tsk("pima")
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)

# tune hyperparameters and fit the final model on the complete data set in one go
at$train(task, row_ids = train_set)

# best performing hyperparameter configuration
at$tuning_result

# all evaluated hyperparameter configuration
as.data.table(at$archive)[, list(batch_nr, cp, classif.ce, resample_result)]

# predict on new data
at$predict(task, row_ids = test_set)
```

### Nested resampling

```{r}
# construct auto tuner with inner resampling
at = auto_tuner(
  method = tnr("random_search"),
  learner = lrn("classif.rpart", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10,
  batch_size = 5
)

# specify outer resampling
resampling_outer = rsmp("cv", folds = 3)

# run nested resampling
rr = resample(tsk("pima"), at, resampling_outer, store_models = TRUE)

# aggregated performance of all outer resampling iterations
rr$aggregate()

# performance scores of the outer resampling
rr$score()[, list(iteration, classif.ce)]

# inner resampling results
extract_inner_tuning_results(rr)[, list(iteration, cp, classif.ce)]

# inner resampling archives
extract_inner_tuning_archives(rr)[, list(iteration, cp, classif.ce)]
```

### Hotstart

```{r}
library("mlr3learners")

# load learner and set search space
learner = lrn("classif.xgboost",
  eta = to_tune(),
  nrounds = to_tune(500, 2500)
)

# hyperparameter tuning on the Pima Indians Diabetes data set
instance = tune(
  method = tnr("grid_search"),
  task = tsk("pima"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  allow_hotstart = TRUE,
  resolution = 5,
  batch_size = 5
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)[, list(batch_nr, eta, nrounds, classif.ce, resample_result)]

# fit the final model on the complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("pima"))
```

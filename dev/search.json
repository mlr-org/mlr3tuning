[{"path":"https://mlr3tuning.mlr-org.com/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marc Becker. Maintainer, author. Michel Lang. Author. Jakob Richter. Author. Bernd Bischl. Author. Daniel Schalk. Author.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Becker M, Lang M, Richter J, Bischl B, Schalk D (2023). mlr3tuning: Hyperparameter Optimization 'mlr3'. https://mlr3tuning.mlr-org.com, https://github.com/mlr-org/mlr3tuning.","code":"@Manual{,   title = {mlr3tuning: Hyperparameter Optimization for 'mlr3'},   author = {Marc Becker and Michel Lang and Jakob Richter and Bernd Bischl and Daniel Schalk},   year = {2023},   note = {https://mlr3tuning.mlr-org.com, https://github.com/mlr-org/mlr3tuning}, }"},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"mlr3tuning-","dir":"","previous_headings":"","what":"Hyperparameter Optimization for mlr3","title":"Hyperparameter Optimization for mlr3","text":"Package website: release | dev mlr3tuning hyperparameter optimization package mlr3 ecosystem. features highly configurable search spaces via paradox package finds optimal hyperparameter configurations mlr3 learner. mlr3tuning works several optimization algorithms e.g. Random Search, Iterated Racing, Bayesian Optimization (mlr3mbo) Hyperband (mlr3hyperband). Moreover, can automatically optimize learners estimate performance optimized models nested resampling. package built optimization framework bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"extension-packages","dir":"","previous_headings":"","what":"Extension packages","title":"Hyperparameter Optimization for mlr3","text":"mlr3tuning extended following packages. mlr3tuningspaces collection search spaces scientific articles commonly used learners. mlr3hyperband adds Hyperband Successive Halving algorithm. mlr3mbo adds Bayesian Optimization methods.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"resources","dir":"","previous_headings":"","what":"Resources","title":"Hyperparameter Optimization for mlr3","text":"several sections hyperparameter optimization mlr3book. Getting started Hyperparameter Optimization. Tune simple classification tree Palmer Penguins data set. Learn Tuning Spaces. Estimate Model Performance Nested Resampling. gallery features collection case studies demos optimization. Learn advanced methods Practical Tuning Series. Optimize rpart classification tree lines code. Tune XGBoost model early stopping. Make us proven search space. Learn hotstarting models. cheatsheet summarizes important functions mlr3tuning.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Hyperparameter Optimization for mlr3","text":"Install last release CRAN: Install development version GitHub:","code":"install.packages(\"mlr3tuning\") remotes::install_github(\"mlr-org/mlr3tuning\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Hyperparameter Optimization for mlr3","text":"optimize cost gamma hyperparameters support vector machine Sonar data set. construct tuning instance ti() function. tuning instance describes tuning problem. select simple grid search optimization algorithm. start tuning, simply pass tuning instance tuner. tuner returns best hyperparameter configuration corresponding measured performance. archive contains evaluated hyperparameter configurations. mlr3viz package visualizes tuning results.  fit final model optimized hyperparameters make predictions new data.","code":"library(\"mlr3verse\")  learner = lrn(\"classif.svm\",   cost  = to_tune(1e-5, 1e5, logscale = TRUE),   gamma = to_tune(1e-5, 1e5, logscale = TRUE),   kernel = \"radial\",   type = \"C-classification\" ) instance = ti(   task = tsk(\"sonar\"),   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measures = msr(\"classif.ce\"),   terminator = trm(\"none\") ) instance ## <TuningInstanceSingleCrit> ## * State:  Not optimized ## * Objective: <ObjectiveTuning:classif.svm_on_sonar> ## * Search Space: ##       id    class     lower    upper nlevels ## 1:  cost ParamDbl -11.51293 11.51293     Inf ## 2: gamma ParamDbl -11.51293 11.51293     Inf ## * Terminator: <TerminatorNone> tuner = tnr(\"grid_search\", resolution = 5) tuner ## <TunerGridSearch>: Grid Search ## * Parameters: resolution=5, batch_size=1 ## * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct ## * Properties: dependencies, single-crit, multi-crit ## * Packages: mlr3tuning tuner$optimize(instance) ##        cost     gamma learner_param_vals  x_domain classif.ce ## 1: 11.51293 -5.756463          <list[4]> <list[2]>  0.1779158 as.data.table(instance$archive)[, .(cost, gamma, classif.ce, batch_nr, resample_result)] ##           cost      gamma classif.ce batch_nr      resample_result ##  1:  11.512925  -5.756463  0.1779158        1 <ResampleResult[21]> ##  2:  11.512925  11.512925  0.4662526        2 <ResampleResult[21]> ##  3:   5.756463   5.756463  0.4662526        3 <ResampleResult[21]> ##  4:  -5.756463  -5.756463  0.4662526        4 <ResampleResult[21]> ##  5: -11.512925   0.000000  0.4662526        5 <ResampleResult[21]> ## ---                                                                ## 21: -11.512925  -5.756463  0.4662526       21 <ResampleResult[21]> ## 22:  11.512925   0.000000  0.4662526       22 <ResampleResult[21]> ## 23:   5.756463  11.512925  0.4662526       23 <ResampleResult[21]> ## 24:  11.512925 -11.512925  0.2498965       24 <ResampleResult[21]> ## 25: -11.512925   5.756463  0.4662526       25 <ResampleResult[21]> library(mlr3viz)  autoplot(instance, type = \"surface\") learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"sonar\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"ArchiveTuning stores evaluated hyperparameter configurations performance scores.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"ArchiveTuning container around data.table::data.table(). row corresponds single evaluation hyperparameter configuration. See section Data Structure information. archive stores additionally mlr3::BenchmarkResult ($benchmark_result) records resampling experiments. experiment corresponds single evaluation hyperparameter configuration. table ($data) benchmark result ($benchmark_result) linked uhash column. archive passed .data.table(), joined automatically.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"data-structure","dir":"Reference","previous_headings":"","what":"Data Structure","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"table ($data) following columns: One column hyperparameter search space ($search_space). One column performance measure ($codomain). x_domain (list()) Lists (transformed) hyperparameter values passed learner. runtime_learners (numeric(1)) Sum training predict times logged learners per mlr3::ResampleResult / evaluation. include potential overhead time. timestamp (POSIXct) Time stamp evaluation logged archive. batch_nr (integer(1)) Hyperparameters evaluated batches. batch unique batch number. uhash (character(1)) Connects hyperparameter configuration resampling experiment stored mlr3::BenchmarkResult.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"analysis","dir":"Reference","previous_headings":"","what":"Analysis","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"analyzing tuning results, recommended pass ArchiveTuning .data.table(). returned data table joined benchmark result adds mlr3::ResampleResult hyperparameter evaluation. archive provides various getters (e.g. $learners()) ease access. getters extract position () unique hash (uhash). complete list getters see methods section. benchmark result ($benchmark_result) allows score hyperparameter configurations different measure. Alternatively, measures can supplied .data.table(). mlr3viz package provides visualizations tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"s-methods","dir":"Reference","previous_headings":"","what":"S3 Methods","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":".data.table.ArchiveTuning(x, unnest = \"x_domain\", exclude_columns = \"uhash\", measures = NULL) Returns tabular view evaluated hyperparameter configurations. ArchiveTuning -> data.table::data.table() x (ArchiveTuning) unnest (character()) Transforms list columns separate columns. Set NULL column unnested. exclude_columns (character()) Exclude columns table. Set NULL column excluded. measures (List mlr3::Measure) Score hyperparameter configurations additional measures.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"bbotk::Archive -> ArchiveTuning","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"benchmark_result (mlr3::BenchmarkResult) Benchmark result.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"bbotk::Archive$add_evals() bbotk::Archive$best() bbotk::Archive$clear() bbotk::Archive$format() bbotk::Archive$nds_selection()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"ArchiveTuning$new() ArchiveTuning$learner() ArchiveTuning$learners() ArchiveTuning$learner_param_vals() ArchiveTuning$predictions() ArchiveTuning$resample_result() ArchiveTuning$print() ArchiveTuning$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$new(search_space, codomain, check_values = TRUE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). codomain (bbotk::Codomain) Specifies codomain objective function .e. set performance measures. Internally created provided mlr3::Measures. check_values (logical(1)) TRUE (default), hyperparameter configurations check validity.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-learner-","dir":"Reference","previous_headings":"","what":"Method learner()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Retrieve mlr3::Learner -th evaluation, position unique hash uhash. uhash mutually exclusive. Learner contain model. Use $learners() get learners models.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$learner(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-learners-","dir":"Reference","previous_headings":"","what":"Method learners()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Retrieve list trained mlr3::Learner objects -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$learners(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-learner-param-vals-","dir":"Reference","previous_headings":"","what":"Method learner_param_vals()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Retrieve param values -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$learner_param_vals(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-predictions-","dir":"Reference","previous_headings":"","what":"Method predictions()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Retrieve list mlr3::Prediction objects -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$predictions(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-resample-result-","dir":"Reference","previous_headings":"","what":"Method resample_result()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Retrieve mlr3::ResampleResult -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$resample_result(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"Printer.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$print()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"... (ignored).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"","code":"ArchiveTuning$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Logging Evaluated Hyperparameter Configurations — ArchiveTuning","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for Automatic Tuning — AutoTuner","title":"Class for Automatic Tuning — AutoTuner","text":"AutoTuner wraps mlr3::Learner augments automatic tuning process given set hyperparameters. auto_tuner() function creates AutoTuner object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Class for Automatic Tuning — AutoTuner","text":"AutoTuner mlr3::Learner wraps another mlr3::Learner performs following steps $train(): hyperparameters wrapped (inner) learner trained training data via resampling. tuning can specified providing Tuner, bbotk::Terminator, search space paradox::ParamSet, mlr3::Resampling mlr3::Measure. best found hyperparameter configuration set hyperparameters wrapped (inner) learner stored $learner. Access tuned hyperparameters via $tuning_result. final model fit complete training data using now parametrized wrapped learner. respective model available via field $learner$model. $predict() AutoTuner just calls predict method wrapped (inner) learner. set timeout disabled fitting final model.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Class for Automatic Tuning — AutoTuner","text":"book chapter automatic tuning. book chapter nested resampling. gallery post tuning nested resampling.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"nested-resampling","dir":"Reference","previous_headings":"","what":"Nested Resampling","title":"Class for Automatic Tuning — AutoTuner","text":"Nested resampling performed passing AutoTuner mlr3::resample() mlr3::benchmark(). access inner resampling results, set store_tuning_instance = TRUE execute mlr3::resample() mlr3::benchmark() store_models = TRUE (see examples). mlr3::Resampling passed AutoTuner meant inner resampling, operating training set arbitrary outer resampling. reason, inner resampling instantiated. instantiated resampling passed, AutoTuner fails row id inner resampling present training set outer resampling.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Class for Automatic Tuning — AutoTuner","text":"mlr3::Learner -> AutoTuner","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class for Automatic Tuning — AutoTuner","text":"instance_args (list()) arguments construction create TuningInstanceSingleCrit. tuner (Tuner) Optimization algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Class for Automatic Tuning — AutoTuner","text":"archive ArchiveTuning Archive TuningInstanceSingleCrit. learner (mlr3::Learner) Trained learner tuning_instance (TuningInstanceSingleCrit) Internally created tuning instance intermediate results. tuning_result (data.table::data.table) Short-cut result TuningInstanceSingleCrit. predict_type (character(1)) Stores currently active predict type, e.g. \"response\". Must element $predict_types. hash (character(1)) Hash (unique identifier) object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Class for Automatic Tuning — AutoTuner","text":"mlr3::Learner$format() mlr3::Learner$help() mlr3::Learner$predict() mlr3::Learner$predict_newdata() mlr3::Learner$reset() mlr3::Learner$train()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class for Automatic Tuning — AutoTuner","text":"AutoTuner$new() AutoTuner$base_learner() AutoTuner$importance() AutoTuner$selected_features() AutoTuner$oob_error() AutoTuner$loglik() AutoTuner$print() AutoTuner$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class for Automatic Tuning — AutoTuner","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$new(   learner,   resampling,   measure = NULL,   terminator,   tuner,   search_space = NULL,   store_tuning_instance = TRUE,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   callbacks = list() )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Automatic Tuning — AutoTuner","text":"learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measure (mlr3::Measure) Measure optimize. NULL, default measure used. terminator (Terminator) Stop criterion tuning process. tuner (Tuner) Optimization algorithm. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_tuning_instance (logical(1)) TRUE (default), stores internally created TuningInstanceSingleCrit intermediate results slot $tuning_instance. store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. callbacks (list CallbackTuning) List callbacks.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-base-learner-","dir":"Reference","previous_headings":"","what":"Method base_learner()","title":"Class for Automatic Tuning — AutoTuner","text":"Extracts base learner nested learner objects like GraphLearner mlr3pipelines. recursive = 0, (tuned) learner returned.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$base_learner(recursive = Inf)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Automatic Tuning — AutoTuner","text":"recursive (integer(1)) Depth recursion multiple nested objects.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Automatic Tuning — AutoTuner","text":"Learner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-importance-","dir":"Reference","previous_headings":"","what":"Method importance()","title":"Class for Automatic Tuning — AutoTuner","text":"importance scores final model.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$importance()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Automatic Tuning — AutoTuner","text":"Named numeric().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-selected-features-","dir":"Reference","previous_headings":"","what":"Method selected_features()","title":"Class for Automatic Tuning — AutoTuner","text":"selected features final model.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$selected_features()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Automatic Tuning — AutoTuner","text":"character().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-oob-error-","dir":"Reference","previous_headings":"","what":"Method oob_error()","title":"Class for Automatic Tuning — AutoTuner","text":"--bag error final model.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$oob_error()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Automatic Tuning — AutoTuner","text":"numeric(1).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-loglik-","dir":"Reference","previous_headings":"","what":"Method loglik()","title":"Class for Automatic Tuning — AutoTuner","text":"log-likelihood final model.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$loglik()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Automatic Tuning — AutoTuner","text":"logLik. Printer.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$print()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Automatic Tuning — AutoTuner","text":"... (ignored).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Class for Automatic Tuning — AutoTuner","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"AutoTuner$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Automatic Tuning — AutoTuner","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Class for Automatic Tuning — AutoTuner","text":"","code":"# Automatic Tuning  # split to train and external set task = tsk(\"penguins\") split = partition(task, ratio = 0.8)  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # create auto tuner at = auto_tuner(   method = tnr(\"random_search\"),   learner = learner,   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  # tune hyperparameters and fit final model at$train(task, row_ids = split$train)  # predict with final model at$predict(task, row_ids = split$test) #> <PredictionClassif> for 69 observations: #>     row_ids     truth  response #>           6    Adelie    Adelie #>          10    Adelie    Adelie #>          13    Adelie    Adelie #> ---                             #>         341 Chinstrap    Adelie #>         342 Chinstrap Chinstrap #>         343 Chinstrap Chinstrap  # show tuning result at$tuning_result #>           cp learner_param_vals  x_domain classif.ce #> 1: -6.408625          <list[2]> <list[1]>  0.1521739  # model slot contains trained learner and tuning instance at$model #> $learner #> <LearnerClassifRpart:classif.rpart>: Classification Tree #> * Model: rpart #> * Parameters: xval=0, cp=0.001647 #> * Packages: mlr3, rpart #> * Predict Types:  [response], prob #> * Feature Types: logical, integer, numeric, factor, ordered #> * Properties: importance, missings, multiclass, selected_features, #>   twoclass, weights #>  #> $tuning_instance #> <TuningInstanceSingleCrit> #> * State:  Optimized #> * Objective: <ObjectiveTuning:classif.rpart_on_penguins> #> * Search Space: #>    id    class    lower     upper nlevels #> 1: cp ParamDbl -9.21034 -2.302585     Inf #> * Terminator: <TerminatorEvals> #> * Result: #>           cp classif.ce #> 1: -6.408625  0.1521739 #> * Archive: #>           cp classif.ce #> 1: -6.408625  0.1521739 #> 2: -6.385343  0.1521739 #> 3: -6.145993  0.1521739 #> 4: -5.281769  0.1521739 #>   # shortcut trained learner at$learner #> <LearnerClassifRpart:classif.rpart>: Classification Tree #> * Model: rpart #> * Parameters: xval=0, cp=0.001647 #> * Packages: mlr3, rpart #> * Predict Types:  [response], prob #> * Feature Types: logical, integer, numeric, factor, ordered #> * Properties: importance, missings, multiclass, selected_features, #>   twoclass, weights  # shortcut tuning instance at$tuning_instance #> <TuningInstanceSingleCrit> #> * State:  Optimized #> * Objective: <ObjectiveTuning:classif.rpart_on_penguins> #> * Search Space: #>    id    class    lower     upper nlevels #> 1: cp ParamDbl -9.21034 -2.302585     Inf #> * Terminator: <TerminatorEvals> #> * Result: #>           cp classif.ce #> 1: -6.408625  0.1521739 #> * Archive: #>           cp classif.ce #> 1: -6.408625  0.1521739 #> 2: -6.385343  0.1521739 #> 3: -6.145993  0.1521739 #> 4: -5.281769  0.1521739   # Nested Resampling  at = auto_tuner(   method = tnr(\"random_search\"),   learner = learner,   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  resampling_outer = rsmp(\"cv\", folds = 3) rr = resample(task, at, resampling_outer, store_models = TRUE)  # retrieve inner tuning results. extract_inner_tuning_results(rr) #>    iteration        cp classif.ce learner_param_vals  x_domain  task_id #> 1:         1 -3.217164 0.09210526          <list[2]> <list[1]> penguins #> 2:         2 -9.092480 0.06578947          <list[2]> <list[1]> penguins #> 3:         3 -2.337910 0.03896104          <list[2]> <list[1]> penguins #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv #> 3: classif.rpart.tuned            cv  # performance scores estimated on the outer resampling rr$score() #>                 task  task_id         learner          learner_id #> 1: <TaskClassif[50]> penguins <AutoTuner[46]> classif.rpart.tuned #> 2: <TaskClassif[50]> penguins <AutoTuner[46]> classif.rpart.tuned #> 3: <TaskClassif[50]> penguins <AutoTuner[46]> classif.rpart.tuned #>            resampling resampling_id iteration              prediction #> 1: <ResamplingCV[20]>            cv         1 <PredictionClassif[20]> #> 2: <ResamplingCV[20]>            cv         2 <PredictionClassif[20]> #> 3: <ResamplingCV[20]>            cv         3 <PredictionClassif[20]> #>    classif.ce #> 1: 0.05217391 #> 2: 0.08695652 #> 3: 0.03508772  # unbiased performance of the final model trained on the full data set rr$aggregate() #> classif.ce  #> 0.05807272"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Tuning Callback — CallbackTuning","title":"Create Tuning Callback — CallbackTuning","text":"Specialized bbotk::CallbackOptimization tuning. Callbacks allow customize behavior processes mlr3tuning. callback_tuning() function creates CallbackTuning. Predefined callbacks stored dictionary mlr_callbacks can retrieved clbk(). information tuning callbacks see callback_tuning().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Create Tuning Callback — CallbackTuning","text":"mlr3misc::Callback -> bbotk::CallbackOptimization -> CallbackTuning","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Create Tuning Callback — CallbackTuning","text":"on_eval_after_design (function()) Stage called design created. Called ObjectiveTuning$eval_many(). on_eval_after_benchmark (function()) Stage called hyperparameter configurations evaluated. Called ObjectiveTuning$eval_many(). on_eval_before_archive (function()) Stage called performance values written archive. Called ObjectiveTuning$eval_many().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Create Tuning Callback — CallbackTuning","text":"mlr3misc::Callback$call() mlr3misc::Callback$format() mlr3misc::Callback$help() mlr3misc::Callback$initialize() mlr3misc::Callback$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Create Tuning Callback — CallbackTuning","text":"CallbackTuning$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Create Tuning Callback — CallbackTuning","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Tuning Callback — CallbackTuning","text":"","code":"CallbackTuning$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Tuning Callback — CallbackTuning","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/CallbackTuning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Tuning Callback — CallbackTuning","text":"","code":"# write archive to disk callback_tuning(\"mlr3tuning.backup\",   on_optimization_end = function(callback, context) {     saveRDS(context$instance$archive, \"archive.rds\")   } ) #> <CallbackTuning:mlr3tuning.backup> #> * Active Stages: on_optimization_end"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluation Context — ContextEval","title":"Evaluation Context — ContextEval","text":"ContextEval allows CallbackTunings access modify data batch hyperparameter configurations evaluated. See section active bindings list modifiable objects. See callback_tuning() list stages access ContextEval.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluation Context — ContextEval","text":"context re-created time new batch hyperparameter configurations evaluated. Changes $objective_tuning, $design $benchmark_result discarded function finished. Modification data table $aggregated_performance written archive. number columns can added.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Evaluation Context — ContextEval","text":"mlr3misc::Context -> ContextEval","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Evaluation Context — ContextEval","text":"objective_tuning ObjectiveTuning.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Evaluation Context — ContextEval","text":"xss (list()) hyperparameter configurations latest batch. Contains values learner scale .e. transformations applied. See $xdt bbotk::ContextOptimization untransformed values. design (data.table::data.table) benchmark design latest batch. benchmark_result (mlr3::BenchmarkResult) benchmark result latest batch. aggregated_performance (data.table::data.table) Aggregated performance scores training time latest batch. data table passed archive. callback can add additional columns also written archive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Evaluation Context — ContextEval","text":"mlr3misc::Context$format() mlr3misc::Context$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Evaluation Context — ContextEval","text":"ContextEval$new() ContextEval$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Evaluation Context — ContextEval","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluation Context — ContextEval","text":"","code":"ContextEval$new(objective_tuning)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluation Context — ContextEval","text":"objective_tuning ObjectiveTuning. id (character(1)) Identifier new callback.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Evaluation Context — ContextEval","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluation Context — ContextEval","text":"","code":"ContextEval$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ContextEval.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluation Context — ContextEval","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for Tuning Objective — ObjectiveTuning","title":"Class for Tuning Objective — ObjectiveTuning","text":"Stores objective function estimates performance hyperparameter configurations. class usually constructed internally TuningInstanceSingleCrit TuningInstanceMultiCrit.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Class for Tuning Objective — ObjectiveTuning","text":"bbotk::Objective -> ObjectiveTuning","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class for Tuning Objective — ObjectiveTuning","text":"task (mlr3::Task). learner (mlr3::Learner). resampling (mlr3::Resampling). measures (list mlr3::Measure). store_models (logical(1)). store_benchmark_result (logical(1)). archive (ArchiveTuning). hotstart_stack (mlr3::HotstartStack). allow_hotstart (logical(1)). keep_hotstart_stack (logical(1)). callbacks (List CallbackTunings).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Class for Tuning Objective — ObjectiveTuning","text":"bbotk::Objective$eval() bbotk::Objective$eval_dt() bbotk::Objective$eval_many() bbotk::Objective$format() bbotk::Objective$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class for Tuning Objective — ObjectiveTuning","text":"ObjectiveTuning$new() ObjectiveTuning$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class for Tuning Objective — ObjectiveTuning","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Objective — ObjectiveTuning","text":"","code":"ObjectiveTuning$new(   task,   learner,   resampling,   measures,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = TRUE,   allow_hotstart = FALSE,   keep_hotstart_stack = FALSE,   archive = NULL,   callbacks = list() )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Tuning Objective — ObjectiveTuning","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (list mlr3::Measure) Measures optimize. store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. keep_hotstart_stack (logical(1)) TRUE, mlr3::HotstartStack kept $objective$hotstart_stack tuning. archive (ArchiveTuning) Reference archive TuningInstanceSingleCrit | TuningInstanceMultiCrit. NULL (default), benchmark result models stored. callbacks (list CallbackTuning) List callbacks.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Class for Tuning Objective — ObjectiveTuning","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Objective — ObjectiveTuning","text":"","code":"ObjectiveTuning$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Tuning Objective — ObjectiveTuning","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for Tuning Algorithms — Tuner","title":"Class for Tuning Algorithms — Tuner","text":"Tuner implements optimization algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Class for Tuning Algorithms — Tuner","text":"Tuner abstract base class implements base functionality tuner must provide. subclass implemented following way: Inherit Tuner. Specify private abstract method $.optimize() use call optimizer. need call instance$eval_batch() evaluate design points. batch evaluation requested TuningInstanceSingleCrit/TuningInstanceMultiCrit object instance, batch possibly executed parallel via mlr3::benchmark(), evaluations stored inside instance$archive. batch evaluation, bbotk::Terminator checked, positive, exception class \"terminated_error\" generated.  later case current batch evaluations still stored instance, numeric scores sent back handling optimizer lost execution control. exception caught select best configuration instance$archive return . Note therefore points specified bbotk::Terminator may evaluated, Terminator checked batch evaluation, -evaluation batch. many depends setting batch size. Overwrite private super-method .assign_result() want decide estimate final configuration instance estimated performance. default behavior : pick best resample-experiment, regarding given measure, assign configuration aggregated performance instance.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"private-methods","dir":"Reference","previous_headings":"","what":"Private Methods","title":"Class for Tuning Algorithms — Tuner","text":".optimize(instance) -> NULL Abstract base method. Implement specify tuning subclass. See details sections. .assign_result(instance) -> NULL Abstract base method. Implement specify final configuration selected. See details sections.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Class for Tuning Algorithms — Tuner","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class for Tuning Algorithms — Tuner","text":"id (character(1)) Identifier object. Used tables, plot text output.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Class for Tuning Algorithms — Tuner","text":"param_set (paradox::ParamSet) Set control parameters. param_classes (character()) Supported parameter classes learner hyperparameters tuner can optimize. Subclasses paradox::Param. properties (character()) Set properties tuner. Must subset mlr_reflections$tuner_properties. packages (character()) Set required packages. Note packages loaded via requireNamespace(), attached. label (character(1)) Label object. Can used tables, plot text output instead ID. man (character(1)) String format [pkg]::[topic] pointing manual page object. referenced help package can opened via method $help().","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class for Tuning Algorithms — Tuner","text":"Tuner$new() Tuner$format() Tuner$print() Tuner$help() Tuner$optimize() Tuner$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class for Tuning Algorithms — Tuner","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Algorithms — Tuner","text":"","code":"Tuner$new(   id = \"tuner\",   param_set,   param_classes,   properties,   packages = character(),   label = NA_character_,   man = NA_character_ )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Tuning Algorithms — Tuner","text":"id (character(1)) Identifier new instance. param_set (paradox::ParamSet) Set control parameters. param_classes (character()) Supported parameter classes learner hyperparameters tuner can optimize. Subclasses paradox::Param. properties (character()) Set properties tuner. Must subset mlr_reflections$tuner_properties. packages (character()) Set required packages. Note packages loaded via requireNamespace(), attached. label (character(1)) Label object. Can used tables, plot text output instead ID. man (character(1)) String format [pkg]::[topic] pointing manual page object. referenced help package can opened via method $help().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-format-","dir":"Reference","previous_headings":"","what":"Method format()","title":"Class for Tuning Algorithms — Tuner","text":"Helper print outputs.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Algorithms — Tuner","text":"","code":"Tuner$format(...)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Tuning Algorithms — Tuner","text":"... (ignored).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Tuning Algorithms — Tuner","text":"(character()).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Class for Tuning Algorithms — Tuner","text":"Print method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Algorithms — Tuner","text":"","code":"Tuner$print()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Tuning Algorithms — Tuner","text":"(character()).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-help-","dir":"Reference","previous_headings":"","what":"Method help()","title":"Class for Tuning Algorithms — Tuner","text":"Opens corresponding help page referenced field $man.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Algorithms — Tuner","text":"","code":"Tuner$help()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-optimize-","dir":"Reference","previous_headings":"","what":"Method optimize()","title":"Class for Tuning Algorithms — Tuner","text":"Performs tuning TuningInstanceSingleCrit TuningInstanceMultiCrit termination. single evaluations written ArchiveTuning resides TuningInstanceSingleCrit/TuningInstanceMultiCrit. result written instance object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Algorithms — Tuner","text":"","code":"Tuner$optimize(inst)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Tuning Algorithms — Tuner","text":"inst (TuningInstanceSingleCrit | TuningInstanceMultiCrit).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Class for Tuning Algorithms — Tuner","text":"data.table::data.table()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Class for Tuning Algorithms — Tuner","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Tuning Algorithms — Tuner","text":"","code":"Tuner$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Tuning Algorithms — Tuner","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"TunerFromOptimizer — TunerFromOptimizer","title":"TunerFromOptimizer — TunerFromOptimizer","text":"Internally used transform bbotk::Optimizer Tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"TunerFromOptimizer — TunerFromOptimizer","text":"mlr3tuning::Tuner -> TunerFromOptimizer","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"TunerFromOptimizer — TunerFromOptimizer","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"TunerFromOptimizer — TunerFromOptimizer","text":"TunerFromOptimizer$new() TunerFromOptimizer$optimize() TunerFromOptimizer$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"TunerFromOptimizer — TunerFromOptimizer","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TunerFromOptimizer — TunerFromOptimizer","text":"","code":"TunerFromOptimizer$new(optimizer, man = NA_character_)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TunerFromOptimizer — TunerFromOptimizer","text":"optimizer bbotk::Optimizer Optimizer called. man (character(1)) String format [pkg]::[topic] pointing manual page object. referenced help package can opened via method $help().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"method-optimize-","dir":"Reference","previous_headings":"","what":"Method optimize()","title":"TunerFromOptimizer — TunerFromOptimizer","text":"Performs tuning TuningInstanceSingleCrit / TuningInstanceMultiCrit termination. single evaluations final results written ArchiveTuning resides TuningInstanceSingleCrit/TuningInstanceMultiCrit. final result returned.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"TunerFromOptimizer — TunerFromOptimizer","text":"","code":"TunerFromOptimizer$optimize(inst)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"TunerFromOptimizer — TunerFromOptimizer","text":"inst (TuningInstanceSingleCrit | TuningInstanceMultiCrit).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"TunerFromOptimizer — TunerFromOptimizer","text":"data.table::data.table.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"TunerFromOptimizer — TunerFromOptimizer","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"TunerFromOptimizer — TunerFromOptimizer","text":"","code":"TunerFromOptimizer$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"TunerFromOptimizer — TunerFromOptimizer","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"TuningInstanceMultiCrit specifies tuning problem Tuners. function ti() creates TuningInstanceMultiCrit function tune() creates instance internally.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"instance contains ObjectiveTuning object encodes black box objective function Tuner optimize. instance allows basic operations querying objective design points ($eval_batch()). operation usually done Tuner. Evaluations hyperparameter configurations performed batches calling mlr3::benchmark() internally. evaluated hyperparameter configurations stored Archive ($archive). batch evaluated, bbotk::Terminator queried remaining budget. available budget exhausted, exception raised, evaluations can performed point . tuner also supposed store final result, consisting  selected hyperparameter configuration associated estimated performance values, calling method instance$assign_result.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"book chapter hyperparameter optimization. book chapter tuning spaces. gallery post tuning svm. mlr3tuningspaces extension package.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"analysis","dir":"Reference","previous_headings":"","what":"Analysis","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"analyzing tuning results, recommended pass ArchiveTuning .data.table(). returned data table joined benchmark result adds mlr3::ResampleResult hyperparameter evaluation. archive provides various getters (e.g. $learners()) ease access. getters extract position () unique hash (uhash). complete list getters see methods section. benchmark result ($benchmark_result) allows score hyperparameter configurations different measure. Alternatively, measures can supplied .data.table(). mlr3viz package provides visualizations tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"bbotk::OptimInstance -> bbotk::OptimInstanceMultiCrit -> TuningInstanceMultiCrit","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"result_learner_param_vals (list()) List param values optimal learner call.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"bbotk::OptimInstance$clear() bbotk::OptimInstance$eval_batch() bbotk::OptimInstance$format() bbotk::OptimInstance$objective_function() bbotk::OptimInstance$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"TuningInstanceMultiCrit$new() TuningInstanceMultiCrit$assign_result() TuningInstanceMultiCrit$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"","code":"TuningInstanceMultiCrit$new(   task,   learner,   resampling,   measures,   terminator,   search_space = NULL,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   allow_hotstart = FALSE,   keep_hotstart_stack = FALSE,   evaluate_default = FALSE,   callbacks = list() )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (list mlr3::Measure) Measures optimize. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. keep_hotstart_stack (logical(1)) TRUE, mlr3::HotstartStack kept $objective$hotstart_stack tuning. evaluate_default (logical(1)) TRUE, learner evaluated hyperparameters set default values start optimization. callbacks (list CallbackTuning) List callbacks.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"method-assign-result-","dir":"Reference","previous_headings":"","what":"Method assign_result()","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"Tuner object writes best found points estimated performance values . internal use.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"","code":"TuningInstanceMultiCrit$assign_result(xdt, ydt, learner_param_vals = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"xdt (data.table::data.table()) Hyperparameter values data.table::data.table(). row one configuration. Contains values search space. Can contain additional columns extra information. ydt (data.table::data.table()) Optimal outcomes, e.g. Pareto front. learner_param_vals (List named list()s) Fixed parameter values learner neither part ","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"","code":"TuningInstanceMultiCrit$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Class for Multi Criteria Tuning — TuningInstanceMultiCrit","text":"","code":"# Hyperparameter optimization on the Palmer Penguins data set task = tsk(\"penguins\")  # Load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # Construct tuning instance instance = ti(   task = task,   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measures = msrs(c(\"classif.ce\", \"time_train\")),   terminator = trm(\"evals\", n_evals = 4) )  # Choose optimization algorithm tuner = tnr(\"random_search\", batch_size = 2)  # Run tuning tuner$optimize(instance) #>           cp learner_param_vals  x_domain classif.ce  time_train #> 1: -3.921740          <list[2]> <list[1]> 0.05809814 0.004333333 #> 2: -4.305706          <list[2]> <list[1]> 0.06689550 0.004000000  # Optimal hyperparameter configurations instance$result #>           cp learner_param_vals  x_domain classif.ce  time_train #> 1: -3.921740          <list[2]> <list[1]> 0.05809814 0.004333333 #> 2: -4.305706          <list[2]> <list[1]> 0.06689550 0.004000000  # Inspect all evaluated configurations as.data.table(instance$archive) #>           cp classif.ce  time_train  x_domain_cp runtime_learners #> 1: -7.646126 0.06689550 0.008666667 0.0004778920            0.037 #> 2: -3.921740 0.05809814 0.004333333 0.0198066037            0.024 #> 3: -4.305706 0.06689550 0.004000000 0.0134913517            0.023 #> 4: -7.449967 0.06689550 0.004333333 0.0005814608            0.036 #>              timestamp batch_nr warnings errors      resample_result #> 1: 2023-01-30 11:16:02        1        0      0 <ResampleResult[21]> #> 2: 2023-01-30 11:16:02        1        0      0 <ResampleResult[21]> #> 3: 2023-01-30 11:16:02        2        0      0 <ResampleResult[21]> #> 4: 2023-01-30 11:16:02        2        0      0 <ResampleResult[21]>"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"TuningInstanceSingleCrit specifies tuning problem Tuners. function ti() creates TuningInstanceSingleCrit function tune() creates instance internally.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"instance contains ObjectiveTuning object encodes black box objective function Tuner optimize. instance allows basic operations querying objective design points ($eval_batch()). operation usually done Tuner. Evaluations hyperparameter configurations performed batches calling mlr3::benchmark() internally. evaluated hyperparameter configurations stored Archive ($archive). batch evaluated, bbotk::Terminator queried remaining budget. available budget exhausted, exception raised, evaluations can performed point . tuner also supposed store final result, consisting  selected hyperparameter configuration associated estimated performance values, calling method instance$assign_result.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"book chapter hyperparameter optimization. book chapter tuning spaces. gallery post tuning svm. mlr3tuningspaces extension package.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"analysis","dir":"Reference","previous_headings":"","what":"Analysis","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"analyzing tuning results, recommended pass ArchiveTuning .data.table(). returned data table joined benchmark result adds mlr3::ResampleResult hyperparameter evaluation. archive provides various getters (e.g. $learners()) ease access. getters extract position () unique hash (uhash). complete list getters see methods section. benchmark result ($benchmark_result) allows score hyperparameter configurations different measure. Alternatively, measures can supplied .data.table(). mlr3viz package provides visualizations tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"bbotk::OptimInstance -> bbotk::OptimInstanceSingleCrit -> TuningInstanceSingleCrit","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"result_learner_param_vals (list()) Param values optimal learner call.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"bbotk::OptimInstance$clear() bbotk::OptimInstance$eval_batch() bbotk::OptimInstance$format() bbotk::OptimInstance$objective_function() bbotk::OptimInstance$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"TuningInstanceSingleCrit$new() TuningInstanceSingleCrit$assign_result() TuningInstanceSingleCrit$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"","code":"TuningInstanceSingleCrit$new(   task,   learner,   resampling,   measure = NULL,   terminator,   search_space = NULL,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   allow_hotstart = FALSE,   keep_hotstart_stack = FALSE,   evaluate_default = FALSE,   callbacks = list() )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measure (mlr3::Measure) Measure optimize. NULL, default measure used. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. keep_hotstart_stack (logical(1)) TRUE, mlr3::HotstartStack kept $objective$hotstart_stack tuning. evaluate_default (logical(1)) TRUE, learner evaluated hyperparameters set default values start optimization. callbacks (list CallbackTuning) List callbacks.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"method-assign-result-","dir":"Reference","previous_headings":"","what":"Method assign_result()","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"Tuner object writes best found point estimated performance value . internal use.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"","code":"TuningInstanceSingleCrit$assign_result(xdt, y, learner_param_vals = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"xdt (data.table::data.table()) Hyperparameter values data.table::data.table(). row one configuration. Contains values search space. Can contain additional columns extra information. y (numeric(1)) Optimal outcome. learner_param_vals (List named list()s) Fixed parameter values learner neither part ","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"","code":"TuningInstanceSingleCrit$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Class for Single Criterion Tuning — TuningInstanceSingleCrit","text":"","code":"# Hyperparameter optimization on the Palmer Penguins data set task = tsk(\"penguins\")  # Load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # Construct tuning instance instance = ti(   task = task,   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measures = msr(\"classif.ce\"),   terminator = trm(\"evals\", n_evals = 4) )  # Choose optimization algorithm tuner = tnr(\"random_search\", batch_size = 2)  # Run tuning tuner$optimize(instance) #>           cp learner_param_vals  x_domain classif.ce #> 1: -5.458028          <list[2]> <list[1]>  0.0610984  # Set optimal hyperparameter configuration to learner learner$param_set$values = instance$result_learner_param_vals  # Train the learner on the full data set learner$train(task)  # Inspect all evaluated configurations as.data.table(instance$archive) #>           cp classif.ce  x_domain_cp runtime_learners           timestamp #> 1: -5.458028  0.0610984 0.0042619541            0.021 2023-01-30 11:16:03 #> 2: -3.265404  0.0610984 0.0381815244            0.022 2023-01-30 11:16:03 #> 3: -5.269113  0.0610984 0.0051481763            0.021 2023-01-30 11:16:03 #> 4: -7.476677  0.0610984 0.0005661356            0.021 2023-01-30 11:16:03 #>    batch_nr warnings errors      resample_result #> 1:        1        0      0 <ResampleResult[21]> #> 2:        1        0      0 <ResampleResult[21]> #> 3:        2        0      0 <ResampleResult[21]> #> 4:        2        0      0 <ResampleResult[21]>"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/as_search_space.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to a Search Space — as_search_space","title":"Convert to a Search Space — as_search_space","text":"Convert object search space.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/as_search_space.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to a Search Space — as_search_space","text":"","code":"as_search_space(x, ...)  # S3 method for Learner as_search_space(x, ...)  # S3 method for ParamSet as_search_space(x, ...)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/as_search_space.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to a Search Space — as_search_space","text":"x () Object convert search space. ... () Additional arguments.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/as_search_space.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to a Search Space — as_search_space","text":"paradox::ParamSet.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for Automatic Tuning — auto_tuner","title":"Function for Automatic Tuning — auto_tuner","text":"AutoTuner wraps mlr3::Learner augments automatic tuning process given set hyperparameters. auto_tuner() function creates AutoTuner object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for Automatic Tuning — auto_tuner","text":"","code":"auto_tuner(   method,   learner,   resampling,   measure = NULL,   term_evals = NULL,   term_time = NULL,   terminator = NULL,   search_space = NULL,   store_tuning_instance = TRUE,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   callbacks = list(),   ... )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for Automatic Tuning — auto_tuner","text":"method (character(1) | Tuner) Key retrieve tuner mlr_tuners dictionary Tuner object. learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measure (mlr3::Measure) Measure optimize. NULL, default measure used. term_evals (integer(1)) Number allowed evaluations. term_time (integer(1)) Maximum allowed time seconds. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_tuning_instance (logical(1)) TRUE (default), stores internally created TuningInstanceSingleCrit intermediate results slot $tuning_instance. store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. callbacks (list CallbackTuning) List callbacks. ... (named list()) Named arguments set parameters tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for Automatic Tuning — auto_tuner","text":"AutoTuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Function for Automatic Tuning — auto_tuner","text":"AutoTuner mlr3::Learner wraps another mlr3::Learner performs following steps $train(): hyperparameters wrapped (inner) learner trained training data via resampling. tuning can specified providing Tuner, bbotk::Terminator, search space paradox::ParamSet, mlr3::Resampling mlr3::Measure. best found hyperparameter configuration set hyperparameters wrapped (inner) learner stored $learner. Access tuned hyperparameters via $tuning_result. final model fit complete training data using now parametrized wrapped learner. respective model available via field $learner$model. $predict() AutoTuner just calls predict method wrapped (inner) learner. set timeout disabled fitting final model.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Function for Automatic Tuning — auto_tuner","text":"book chapter automatic tuning. book chapter nested resampling. gallery post tuning nested resampling.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"nested-resampling","dir":"Reference","previous_headings":"","what":"Nested Resampling","title":"Function for Automatic Tuning — auto_tuner","text":"Nested resampling performed passing AutoTuner mlr3::resample() mlr3::benchmark(). access inner resampling results, set store_tuning_instance = TRUE execute mlr3::resample() mlr3::benchmark() store_models = TRUE (see examples). mlr3::Resampling passed AutoTuner meant inner resampling, operating training set arbitrary outer resampling. reason, inner resampling instantiated. instantiated resampling passed, AutoTuner fails row id inner resampling present training set outer resampling.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for Automatic Tuning — auto_tuner","text":"","code":"at = auto_tuner(   method = tnr(\"random_search\"),   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  at$train(tsk(\"pima\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/callback_tuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Tuning Callback — callback_tuning","title":"Create Tuning Callback — callback_tuning","text":"Function create CallbackTuning. Predefined callbacks stored dictionary mlr_callbacks can retrieved clbk(). Tuning callbacks can called different stages tuning process. stages prefixed on_*.   See also section parameters information stages. tuning callback works bbotk::ContextOptimization ContextEval.","code":"Start Tuning      - on_optimization_begin     Start Tuner Batch          - on_optimizer_before_eval         Start Evaluation              - on_eval_after_design              - on_eval_after_benchmark              - on_eval_before_archive         End Evaluation          - on_optimizer_after_eval     End Tuner Batch      - on_result      - on_optimization_end End Tuning"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/callback_tuning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Tuning Callback — callback_tuning","text":"","code":"callback_tuning(   id,   label = NA_character_,   man = NA_character_,   on_optimization_begin = NULL,   on_optimizer_before_eval = NULL,   on_eval_after_design = NULL,   on_eval_after_benchmark = NULL,   on_eval_before_archive = NULL,   on_optimizer_after_eval = NULL,   on_result = NULL,   on_optimization_end = NULL )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/callback_tuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Tuning Callback — callback_tuning","text":"id (character(1)) Identifier new instance. label (character(1)) Label new instance. man (character(1)) String format [pkg]::[topic] pointing manual page object. referenced help package can opened via method $help(). on_optimization_begin (function()) Stage called beginning optimization. Called Optimizer$optimize(). context available bbotk::ContextOptimization. on_optimizer_before_eval (function()) Stage called optimizer proposes points. Called OptimInstance$eval_batch(). context available bbotk::ContextOptimization. on_eval_after_design (function()) Stage called design created. Called ObjectiveTuning$eval_many(). context available ContextEval. on_eval_after_benchmark (function()) Stage called hyperparameter configurations evaluated. Called ObjectiveTuning$eval_many(). context available ContextEval. on_eval_before_archive (function()) Stage called performance values written archive. Called ObjectiveTuning$eval_many(). context available ContextEval. on_optimizer_after_eval (function()) Stage called points evaluated. Called OptimInstance$eval_batch(). context available bbotk::ContextOptimization. on_result (function()) Stage called result written. Called OptimInstance$assign_result(). context available bbotk::ContextOptimization. on_optimization_end (function()) Stage called end optimization. Called Optimizer$optimize(). context available bbotk::ContextOptimization.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/callback_tuning.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Tuning Callback — callback_tuning","text":"implementing callback, functions must two arguments named callback context. callback can write data state ($state), e.g. settings affect callback . Avoid writing large data state. can slow tuning process evaluation configurations parallelized. Tuning callbacks access two different contexts depending stage. stages on_eval_after_design, on_eval_after_benchmark, on_eval_before_archive access ContextEval. context can used customize evaluation batch hyperparameter configurations. Changes state callback lost evaluation batch changes tuning instance tuner possible. Persistent data written archive via $aggregated_performance (see ContextEval). stages access ContextOptimization. context can used modify tuning instance, archive, tuner final result. two different contexts evaluation can parallelized .e. multiple instances ContextEval exists different workers time.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/callback_tuning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Tuning Callback — callback_tuning","text":"","code":"# write archive to disk callback_tuning(\"mlr3tuning.backup\",   on_optimization_end = function(callback, context) {     saveRDS(context$instance$archive, \"archive.rds\")   } ) #> <CallbackTuning:mlr3tuning.backup> #> * Active Stages: on_optimization_end"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Inner Tuning Archives — extract_inner_tuning_archives","title":"Extract Inner Tuning Archives — extract_inner_tuning_archives","text":"Extract inner tuning archives nested resampling. Implemented mlr3::ResampleResult mlr3::BenchmarkResult. function iterates AutoTuner objects binds tuning archives data.table::data.table(). AutoTuner must initialized store_tuning_instance = TRUE mlr3::resample() mlr3::benchmark() must called store_models = TRUE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Inner Tuning Archives — extract_inner_tuning_archives","text":"","code":"extract_inner_tuning_archives(   x,   unnest = \"x_domain\",   exclude_columns = \"uhash\" )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Inner Tuning Archives — extract_inner_tuning_archives","text":"x (mlr3::ResampleResult | mlr3::BenchmarkResult). unnest (character()) Transforms list columns separate columns. default, x_domain unnested. Set NULL column unnested. exclude_columns (character()) Exclude columns result table. Set NULL column excluded.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Inner Tuning Archives — extract_inner_tuning_archives","text":"data.table::data.table().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"data-structure","dir":"Reference","previous_headings":"","what":"Data structure","title":"Extract Inner Tuning Archives — extract_inner_tuning_archives","text":"returned data table following columns: experiment (integer(1)) Index, giving according row number original benchmark grid. iteration (integer(1)) Iteration outer resampling. One column hyperparameter search spaces. One column performance measure. runtime_learners (numeric(1)) Sum training predict times logged learners per mlr3::ResampleResult / evaluation. include potential overhead time. timestamp (POSIXct) Time stamp evaluation logged archive. batch_nr (integer(1)) Hyperparameters evaluated batches. batch unique batch number. x_domain (list()) List transformed hyperparameter values. default column unnested. x_domain_* () Separate column transformed hyperparameter. resample_result (mlr3::ResampleResult) Resample result inner resampling. task_id (character(1)). learner_id (character(1)). resampling_id (character(1)).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Inner Tuning Archives — extract_inner_tuning_archives","text":"","code":"# Nested Resampling on Palmer Penguins Data Set  learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # create auto tuner at = auto_tuner(   method = tnr(\"random_search\"),   learner = learner,   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  resampling_outer = rsmp(\"cv\", folds = 2) rr = resample(tsk(\"iris\"), at, resampling_outer, store_models = TRUE)  # extract inner archives extract_inner_tuning_archives(rr) #>    iteration        cp classif.ce  x_domain_cp runtime_learners #> 1:         1 -8.410702       0.08 0.0002224737            0.007 #> 2:         1 -7.874385       0.08 0.0003803628            0.007 #> 3:         1 -5.475264       0.08 0.0041891218            0.007 #> 4:         1 -7.054511       0.08 0.0008635048            0.006 #> 5:         2 -7.340622       0.04 0.0006486472            0.006 #> 6:         2 -4.870036       0.04 0.0076730927            0.006 #> 7:         2 -9.114627       0.04 0.0001100444            0.006 #> 8:         2 -6.902996       0.04 0.0010047711            0.006 #>              timestamp batch_nr warnings errors      resample_result task_id #> 1: 2023-01-30 11:16:05        1        0      0 <ResampleResult[21]>    iris #> 2: 2023-01-30 11:16:05        2        0      0 <ResampleResult[21]>    iris #> 3: 2023-01-30 11:16:05        3        0      0 <ResampleResult[21]>    iris #> 4: 2023-01-30 11:16:05        4        0      0 <ResampleResult[21]>    iris #> 5: 2023-01-30 11:16:06        1        0      0 <ResampleResult[21]>    iris #> 6: 2023-01-30 11:16:06        2        0      0 <ResampleResult[21]>    iris #> 7: 2023-01-30 11:16:06        3        0      0 <ResampleResult[21]>    iris #> 8: 2023-01-30 11:16:06        4        0      0 <ResampleResult[21]>    iris #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv #> 3: classif.rpart.tuned            cv #> 4: classif.rpart.tuned            cv #> 5: classif.rpart.tuned            cv #> 6: classif.rpart.tuned            cv #> 7: classif.rpart.tuned            cv #> 8: classif.rpart.tuned            cv"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Inner Tuning Results — extract_inner_tuning_results","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"Extract inner tuning results nested resampling. Implemented mlr3::ResampleResult mlr3::BenchmarkResult.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"","code":"extract_inner_tuning_results(x, tuning_instance, ...)  # S3 method for ResampleResult extract_inner_tuning_results(x, tuning_instance = FALSE, ...)  # S3 method for BenchmarkResult extract_inner_tuning_results(x, tuning_instance = FALSE, ...)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"x (mlr3::ResampleResult | mlr3::BenchmarkResult). tuning_instance (logical(1)) TRUE, tuning instances added table. ... () Additional arguments.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"data.table::data.table().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"function iterates AutoTuner objects binds tuning results data.table::data.table(). AutoTuner must initialized store_tuning_instance = TRUE mlr3::resample() mlr3::benchmark() must called store_models = TRUE. Optionally, tuning instance can added iteration.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"data-structure","dir":"Reference","previous_headings":"","what":"Data structure","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"returned data table following columns: experiment (integer(1)) Index, giving according row number original benchmark grid. iteration (integer(1)) Iteration outer resampling. One column hyperparameter search spaces. One column performance measure. learner_param_vals (list()) Hyperparameter values used learner. Includes fixed proposed hyperparameter values. x_domain (list()) List transformed hyperparameter values. tuning_instance (TuningInstanceSingleCrit | TuningInstanceMultiCrit) Optionally, tuning instances. task_id (character(1)). learner_id (character(1)). resampling_id (character(1)).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Inner Tuning Results — extract_inner_tuning_results","text":"","code":"# Nested Resampling on Palmer Penguins Data Set  learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # create auto tuner at = auto_tuner(   method = tnr(\"random_search\"),   learner = learner,   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  resampling_outer = rsmp(\"cv\", folds = 2) rr = resample(tsk(\"iris\"), at, resampling_outer, store_models = TRUE)  # extract inner results extract_inner_tuning_results(rr) #>    iteration        cp classif.ce learner_param_vals  x_domain task_id #> 1:         1 -6.591976       0.00          <list[2]> <list[1]>    iris #> 2:         2 -2.657950       0.08          <list[2]> <list[1]>    iris #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning-package.html","id":null,"dir":"Reference","previous_headings":"","what":"mlr3tuning: Hyperparameter Optimization for 'mlr3' — mlr3tuning-package","title":"mlr3tuning: Hyperparameter Optimization for 'mlr3' — mlr3tuning-package","text":"Hyperparameter optimization package 'mlr3' ecosystem. features highly configurable search spaces via 'paradox' package finds optimal hyperparameter configurations 'mlr3' learner. 'mlr3tuning' works several optimization algorithms e.g. Random Search, Iterated Racing, Bayesian Optimization ('mlr3mbo') Hyperband ('mlr3hyperband'). Moreover, can automatically optimize learners estimate performance optimized models nested resampling.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mlr3tuning: Hyperparameter Optimization for 'mlr3' — mlr3tuning-package","text":"Maintainer: Marc Becker marcbecker@posteo.de (ORCID) Authors: Michel Lang michellang@gmail.com (ORCID) Jakob Richter jakob1richter@gmail.com (ORCID) Bernd Bischl bernd_bischl@gmx.net (ORCID) Daniel Schalk daniel.schalk@stat.uni-muenchen.de (ORCID)","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning.backup.html","id":null,"dir":"Reference","previous_headings":"","what":"Backup Benchmark Result Callback — mlr3tuning.backup","title":"Backup Benchmark Result Callback — mlr3tuning.backup","text":"CallbackTuning writes mlr3::BenchmarkResult batch disk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning.backup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Backup Benchmark Result Callback — mlr3tuning.backup","text":"","code":"clbk(\"mlr3tuning.backup\", path = \"backup.rds\") #> <CallbackTuning:mlr3tuning.backup>: Backup Benchmark Result Callback #> * Active Stages: on_optimizer_after_eval, on_optimization_begin  # tune classification tree on the pima data set instance = tune(   method = tnr(\"random_search\"),   task = tsk(\"pima\"),   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp(\"cv\", folds = 3),   measures = msr(\"classif.ce\"),   term_evals = 4,   batch_size = 2,   callbacks = clbk(\"mlr3tuning.backup\", path = tempfile(fileext = \".rds\")) )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning.early_stopping.html","id":null,"dir":"Reference","previous_headings":"","what":"Early Stopping Callback — mlr3tuning.early_stopping","title":"Early Stopping Callback — mlr3tuning.early_stopping","text":"CallbackTuning integrates early stopping hyperparameter tuning XGBoost learner. Early stopping estimates optimal number trees (nrounds) given hyperparameter configuration. Since early stopping performed resampling iteration, several optimal nrounds values. callback writes maximum value archive max_nrounds column. best hyperparameter configuration (instance$result_learner_param_vals), value nrounds replaced max_nrounds early stopping deactivated.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning.early_stopping.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Early Stopping Callback — mlr3tuning.early_stopping","text":"Currently, callback work GraphLearners package mlr3pipelines. callback compatible AutoTuner. final model fitted best hyperparameter configuration max_nrounds .e. early stopping performed.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning.early_stopping.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Early Stopping Callback — mlr3tuning.early_stopping","text":"gallery post early stopping XGBoost.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning.early_stopping.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Early Stopping Callback — mlr3tuning.early_stopping","text":"","code":"clbk(\"mlr3tuning.early_stopping\") #> <CallbackTuning:mlr3tuning.early_stopping>: Early Stopping Callback #> * Active Stages: on_eval_before_archive, on_eval_after_benchmark, #>   on_result, on_optimization_begin # \\donttest{ if (requireNamespace(\"mlr3learners\") && requireNamespace(\"xgboost\") ) {   library(mlr3learners)    # activate early stopping on the test set and set search space   learner = lrn(\"classif.xgboost\",     eta = to_tune(1e-02, 1e-1, logscale = TRUE),     early_stopping_rounds = 5,     nrounds = 100,     early_stopping_set = \"test\")    # tune xgboost on the pima data set   instance = tune(     method = tnr(\"random_search\"),     task = tsk(\"pima\"),     learner = learner,     resampling = rsmp(\"cv\", folds = 3),     measures = msr(\"classif.ce\"),     term_evals = 10,     callbacks = clbk(\"mlr3tuning.early_stopping\")   ) } #> Loading required namespace: mlr3learners #> Loading required namespace: xgboost # }"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":null,"dir":"Reference","previous_headings":"","what":"Dictionary of Tuners — mlr_tuners","title":"Dictionary of Tuners — mlr_tuners","text":"simple mlr3misc::Dictionary storing objects class Tuner. tuner associated help page, see mlr_tuners_[id]. dictionary can get populated additional tuners add-packages. convenient way retrieve construct tuner, see tnr()/tnrs().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dictionary of Tuners — mlr_tuners","text":"R6::R6Class object inheriting mlr3misc::Dictionary.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Dictionary of Tuners — mlr_tuners","text":"See mlr3misc::Dictionary.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"s-methods","dir":"Reference","previous_headings":"","what":"S3 methods","title":"Dictionary of Tuners — mlr_tuners","text":".data.table(dict, ..., objects = FALSE)mlr3misc::Dictionary -> data.table::data.table() Returns data.table::data.table() fields \"key\", \"label\", \"param_classes\", \"properties\" \"packages\" columns. objects set TRUE, constructed objects returned list column named object.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dictionary of Tuners — mlr_tuners","text":"","code":"as.data.table(mlr_tuners) #>              key                                           label #> 1:         cmaes Covariance Matrix Adaptation Evolution Strategy #> 2: design_points                                   Design Points #> 3:         gensa                 Generalized Simulated Annealing #> 4:   grid_search                                     Grid Search #> 5:         irace                                 Iterated Racing #> 6:        nloptr                         Non-linear Optimization #> 7: random_search                                   Random Search #>                                   param_classes #> 1:                                     ParamDbl #> 2: ParamLgl,ParamInt,ParamDbl,ParamFct,ParamUty #> 3:                                     ParamDbl #> 4:          ParamLgl,ParamInt,ParamDbl,ParamFct #> 5:          ParamDbl,ParamInt,ParamFct,ParamLgl #> 6:                                     ParamDbl #> 7:          ParamLgl,ParamInt,ParamDbl,ParamFct #>                             properties                packages #> 1:                         single-crit mlr3tuning,bbotk,adagio #> 2: dependencies,single-crit,multi-crit        mlr3tuning,bbotk #> 3:                         single-crit  mlr3tuning,bbotk,GenSA #> 4: dependencies,single-crit,multi-crit              mlr3tuning #> 5:            dependencies,single-crit  mlr3tuning,bbotk,irace #> 6:                         single-crit mlr3tuning,bbotk,nloptr #> 7: dependencies,single-crit,multi-crit        mlr3tuning,bbotk mlr_tuners$get(\"random_search\") #> <TunerRandomSearch>: Random Search #> * Parameters: batch_size=1 #> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct #> * Properties: dependencies, single-crit, multi-crit #> * Packages: mlr3tuning, bbotk tnr(\"random_search\") #> <TunerRandomSearch>: Random Search #> * Parameters: batch_size=1 #> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct #> * Properties: dependencies, single-crit, multi-crit #> * Packages: mlr3tuning, bbotk"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"Subclass Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Calls adagio::pureCMAES() package adagio.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"Hansen N (2016). “CMA Evolution Strategy: Tutorial.” 1604.00772.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"cmaes\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"control-parameters","dir":"Reference","previous_headings":"","what":"Control Parameters","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"start_values character(1) Create random start values based center search space? latter case, center parameters trafo applied. meaning control parameters, see adagio::pureCMAES(). Note removed control parameters refer termination algorithm terminators allow obtain behavior.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"Tuner based bbotk::OptimizerCmaes can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerCmaes","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"TunerCmaes$new() TunerCmaes$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"","code":"TunerCmaes$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"","code":"TunerCmaes$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy — mlr_tuners_cmaes","text":"","code":"# Hyperparameter Optimization  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE),   minsplit = to_tune(p_dbl(2, 128, trafo = as.integer)),   minbucket = to_tune(p_dbl(1, 64, trafo = as.integer)) )  # run hyperparameter tuning on the Palmer Penguins data set instance = tune(   method = tnr(\"cmaes\"),   task = tsk(\"penguins\"),   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10)  # best performing hyperparameter configuration instance$result #>           cp minsplit minbucket learner_param_vals  x_domain classif.ce #> 1: -5.450113        2         1          <list[4]> <list[3]> 0.02608696  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp   minsplit minbucket classif.ce  x_domain_cp x_domain_minsplit #>  1: -2.302585  29.335627  36.20970 0.06086957 0.1000000000                29 #>  2: -2.302585 117.139476  36.85380 0.06086957 0.1000000000               117 #>  3: -8.067102   2.000000  64.00000 0.11304348 0.0003136911                 2 #>  4: -7.174879   2.000000  34.52199 0.06086957 0.0007655781                 2 #>  5: -5.450113   2.000000   1.00000 0.02608696 0.0042958189                 2 #>  6: -6.726094   2.000000  47.21739 0.06086957 0.0011992083                 2 #>  7: -5.723954  98.410131   1.00000 0.06086957 0.0032667687                98 #>  8: -3.996986 107.943122  22.73753 0.06086957 0.0183709269               107 #>  9: -4.041377   3.470059  29.79230 0.06086957 0.0175732492                 3 #> 10: -7.362705  79.891607  11.73757 0.06086957 0.0006344798                79 #>     x_domain_minbucket runtime_learners           timestamp batch_nr warnings #>  1:                 36            0.007 2023-01-30 11:16:15        1        0 #>  2:                 36            0.007 2023-01-30 11:16:15        2        0 #>  3:                 64            0.007 2023-01-30 11:16:15        3        0 #>  4:                 34            0.007 2023-01-30 11:16:15        4        0 #>  5:                  1            0.008 2023-01-30 11:16:15        5        0 #>  6:                 47            0.007 2023-01-30 11:16:15        6        0 #>  7:                  1            0.007 2023-01-30 11:16:16        7        0 #>  8:                 22            0.008 2023-01-30 11:16:16        8        0 #>  9:                 29            0.008 2023-01-30 11:16:16        9        0 #> 10:                 11            0.007 2023-01-30 11:16:16       10        0 #>     errors      resample_result #>  1:      0 <ResampleResult[21]> #>  2:      0 <ResampleResult[21]> #>  3:      0 <ResampleResult[21]> #>  4:      0 <ResampleResult[21]> #>  5:      0 <ResampleResult[21]> #>  6:      0 <ResampleResult[21]> #>  7:      0 <ResampleResult[21]> #>  8:      0 <ResampleResult[21]> #>  9:      0 <ResampleResult[21]> #> 10:      0 <ResampleResult[21]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"penguins\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"Subclass tuning w.r.t. fixed design points. simply search set points fully specified user. points design evaluated order given.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"design_points\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"Tuner based bbotk::OptimizerDesignPoints can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"batch_size integer(1) Maximum number configurations try batch. design data.table::data.table Design points try search, one per row.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerDesignPoints","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"TunerDesignPoints$new() TunerDesignPoints$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"","code":"TunerDesignPoints$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"","code":"TunerDesignPoints$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Design Points — mlr_tuners_design_points","text":"","code":"# Hyperparameter Optimization  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1),   minsplit = to_tune(2, 128),   minbucket = to_tune(1, 64) )  # create design design = mlr3misc::rowwise_table(   ~cp,   ~minsplit,  ~minbucket,   0.1,   2,          64,   0.01,  64,         32,   0.001, 128,        1 )  # run hyperparameter tuning on the Palmer Penguins data set instance = tune(   method = tnr(\"design_points\", design = design),   task = tsk(\"penguins\"),   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\") )  # best performing hyperparameter configuration instance$result #>      cp minsplit minbucket learner_param_vals  x_domain classif.ce #> 1: 0.01       64        32          <list[4]> <list[3]> 0.06086957  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>       cp minsplit minbucket classif.ce x_domain_cp x_domain_minsplit #> 1: 0.100        2        64 0.12173913       0.100                 2 #> 2: 0.010       64        32 0.06086957       0.010                64 #> 3: 0.001      128         1 0.06086957       0.001               128 #>    x_domain_minbucket runtime_learners           timestamp batch_nr warnings #> 1:                 64            0.007 2023-01-30 11:16:17        1        0 #> 2:                 32            0.007 2023-01-30 11:16:17        2        0 #> 3:                  1            0.007 2023-01-30 11:16:17        3        0 #>    errors      resample_result #> 1:      0 <ResampleResult[21]> #> 2:      0 <ResampleResult[21]> #> 3:      0 <ResampleResult[21]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"penguins\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"Subclass generalized simulated annealing tuning. Calls GenSA::GenSA() package GenSA.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"Tsallis C, Stariolo DA (1996). “Generalized simulated annealing.” Physica : Statistical Mechanics Applications, 233(1-2), 395--406. doi:10.1016/s0378-4371(96)00271-3 . Xiang Y, Gubian S, Suomela B, Hoeng J (2013). “Generalized Simulated Annealing Global Optimization: GenSA Package.” R Journal, 5(1), 13. doi:10.32614/rj-2013-002 .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"contrast GenSA::GenSA() defaults, set smooth = FALSE default.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"gensa\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"Tuner based bbotk::OptimizerGenSA can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"smooth logical(1) temperature numeric(1) acceptance.param numeric(1) verbose logical(1) trace.mat logical(1) meaning control parameters, see GenSA::GenSA(). Note removed control parameters refer termination algorithm terminators allow obtain behavior. contrast GenSA::GenSA() defaults, set trace.mat = FALSE. Note GenSA::GenSA() uses smooth = TRUE default. case using optimizer Hyperparameter Optimization may want set smooth = FALSE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerGenSA","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"TunerGenSA$new() TunerGenSA$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"","code":"TunerGenSA$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"","code":"TunerGenSA$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Generalized Simulated Annealing — mlr_tuners_gensa","text":"","code":"# Hyperparameter Optimization  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # run hyperparameter tuning on the Palmer Penguins data set instance = tune(   method = \"gensa\",   task = tsk(\"penguins\"),   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10 ) #> Warning: one-dimensional optimization by Nelder-Mead is unreliable: #> use \"Brent\" or optimize() directly  # best performing hyperparameter configuration instance$result #>           cp learner_param_vals  x_domain classif.ce #> 1: -3.227331          <list[2]> <list[1]> 0.05217391  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -3.227331 0.05217391 0.0396632377            0.008 2023-01-30 11:16:17 #>  2: -7.264758 0.06086957 0.0006997707            0.007 2023-01-30 11:16:17 #>  3: -5.075283 0.06086957 0.0062493155            0.009 2023-01-30 11:16:17 #>  4: -3.227331 0.05217391 0.0396632377            0.007 2023-01-30 11:16:18 #>  5: -3.227331 0.05217391 0.0396632377            0.008 2023-01-30 11:16:18 #>  6: -3.227331 0.05217391 0.0396632377            0.007 2023-01-30 11:16:18 #>  7: -2.904597 0.05217391 0.0547708331            0.008 2023-01-30 11:16:18 #>  8: -3.550064 0.05217391 0.0287228136            0.007 2023-01-30 11:16:18 #>  9: -3.388697 0.05217391 0.0337526263            0.007 2023-01-30 11:16:18 #> 10: -3.065964 0.05217391 0.0466088894            0.008 2023-01-30 11:16:18 #>     batch_nr warnings errors      resample_result #>  1:        1        0      0 <ResampleResult[21]> #>  2:        2        0      0 <ResampleResult[21]> #>  3:        3        0      0 <ResampleResult[21]> #>  4:        4        0      0 <ResampleResult[21]> #>  5:        5        0      0 <ResampleResult[21]> #>  6:        6        0      0 <ResampleResult[21]> #>  7:        7        0      0 <ResampleResult[21]> #>  8:        8        0      0 <ResampleResult[21]> #>  9:        9        0      0 <ResampleResult[21]> #> 10:       10        0      0 <ResampleResult[21]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"penguins\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"Subclass grid search tuning.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"grid constructed Cartesian product discretized values per parameter, see paradox::generate_design_grid(). learner supports hotstarting, grid sorted hotstart parameter (see also mlr3::HotstartStack). , points grid evaluated  random order.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"grid_search\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"control-parameters","dir":"Reference","previous_headings":"","what":"Control Parameters","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"resolution integer(1) Resolution grid, see paradox::generate_design_grid(). param_resolutions named integer() Resolution per parameter, named parameter ID, see paradox::generate_design_grid(). batch_size integer(1) Maximum number points try batch.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"Tuner based bbotk::OptimizerGridSearch can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"mlr3tuning::Tuner -> TunerGridSearch","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$optimize() mlr3tuning::Tuner$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"TunerGridSearch$new() TunerGridSearch$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"","code":"TunerGridSearch$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"","code":"TunerGridSearch$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Grid Search — mlr_tuners_grid_search","text":"","code":"# Hyperparameter Optimization  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # run hyperparameter tuning on the Palmer Penguins data set instance = tune(   method = \"grid_search\",   task = tsk(\"penguins\"),   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10 )  # best performing hyperparameter configuration instance$result #>           cp learner_param_vals  x_domain classif.ce #> 1: -2.302585          <list[2]> <list[1]> 0.06956522  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -2.302585 0.06956522 0.1000000000            0.007 2023-01-30 11:16:19 #>  2: -3.837642 0.06956522 0.0215443469            0.008 2023-01-30 11:16:19 #>  3: -6.140227 0.06956522 0.0021544347            0.008 2023-01-30 11:16:19 #>  4: -3.070113 0.06956522 0.0464158883            0.007 2023-01-30 11:16:19 #>  5: -6.907755 0.06956522 0.0010000000            0.007 2023-01-30 11:16:19 #>  6: -9.210340 0.06956522 0.0001000000            0.008 2023-01-30 11:16:19 #>  7: -4.605170 0.06956522 0.0100000000            0.007 2023-01-30 11:16:19 #>  8: -8.442812 0.06956522 0.0002154435            0.010 2023-01-30 11:16:19 #>  9: -7.675284 0.06956522 0.0004641589            0.008 2023-01-30 11:16:19 #> 10: -5.372699 0.06956522 0.0046415888            0.007 2023-01-30 11:16:19 #>     batch_nr warnings errors      resample_result #>  1:        1        0      0 <ResampleResult[21]> #>  2:        2        0      0 <ResampleResult[21]> #>  3:        3        0      0 <ResampleResult[21]> #>  4:        4        0      0 <ResampleResult[21]> #>  5:        5        0      0 <ResampleResult[21]> #>  6:        6        0      0 <ResampleResult[21]> #>  7:        7        0      0 <ResampleResult[21]> #>  8:        8        0      0 <ResampleResult[21]> #>  9:        9        0      0 <ResampleResult[21]> #> 10:       10        0      0 <ResampleResult[21]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"penguins\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Subclass iterated racing. Calls irace::irace() package irace.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Lopez-Ibanez M, Dubois-Lacoste J, Caceres LP, Birattari M, Stuetzle T (2016). “irace package: Iterated racing automatic algorithm configuration.” Operations Research Perspectives, 3, 43--58. doi:10.1016/j.orp.2016.09.002 .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"irace\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"control-parameters","dir":"Reference","previous_headings":"","what":"Control Parameters","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"n_instances integer(1) Number resampling instances. meaning parameters, see irace::defaultScenario(). Note removed control parameters refer termination algorithm. Use TerminatorEvals instead. terminators work TunerIrace.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"archive","dir":"Reference","previous_headings":"","what":"Archive","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"ArchiveTuning holds following additional columns: \"race\" (integer(1)) Race iteration. \"step\" (integer(1)) Step number race. \"instance\" (integer(1)) Identifies resampling instances across races steps. \"configuration\" (integer(1)) Identifies configurations across races steps.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"result","dir":"Reference","previous_headings":"","what":"Result","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"tuning result (instance$result) best performing elite final race. reported performance average performance estimated  used instances.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Tuner based bbotk::OptimizerIrace can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerIrace","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"TunerIrace$new() TunerIrace$optimize() TunerIrace$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"","code":"TunerIrace$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"method-optimize-","dir":"Reference","previous_headings":"","what":"Method optimize()","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"Performs tuning TuningInstanceSingleCrit termination. single evaluations final results written ArchiveTuning resides TuningInstanceSingleCrit. final result returned.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"","code":"TunerIrace$optimize(inst)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"inst (TuningInstanceSingleCrit).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"data.table::data.table.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"","code":"TunerIrace$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Iterated Racing. — mlr_tuners_irace","text":"","code":"# retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)) # \\donttest{ # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"irace\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 42 ) #> # 2023-01-30 11:16:20 UTC: Initialization #> # Elitist race #> # Elitist new instances: 1 #> # Elitist limit: 2 #> # nbIterations: 2 #> # minNbSurvival: 2 #> # nbParameters: 1 #> # seed: 841730813 #> # confidence level: 0.95 #> # budget: 42 #> # mu: 5 #> # deterministic: FALSE #>  #> # 2023-01-30 11:16:20 UTC: Iteration 1 of 2 #> # experimentsUsedSoFar: 0 #> # remainingBudget: 42 #> # currentBudget: 21 #> # nbConfigurations: 3 #> # Markers: #>      x No test is performed. #>      c Configurations are discarded only due to capping. #>      - The test is performed and some configurations are discarded. #>      = The test is performed but no configuration is discarded. #>      ! The test is performed and configurations could be discarded but elite configurations are preserved. #>      . All alive configurations are elite and nothing is discarded #>  #> +-+-----------+-----------+-----------+----------------+-----------+--------+-----+----+------+ #> | |   Instance|      Alive|       Best|       Mean best| Exp so far|  W time|  rho|KenW|  Qvar| #> +-+-----------+-----------+-----------+----------------+-----------+--------+-----+----+------+ #> |x|          1|          3|          3|    0.2226562500|          3|00:00:00|   NA|  NA|    NA| #> |x|          2|          3|          3|    0.2343750000|          6|00:00:00|+0.50|0.75|0.4178| #> |x|          3|          3|          3|    0.2460937500|          9|00:00:00|+0.45|0.64|0.3010| #> |x|          4|          3|          3|    0.2451171875|         12|00:00:00|+0.52|0.64|0.2314| #> |-|          5|          2|          3|    0.2421875000|         15|00:00:00|+0.50|0.60|0.1500| #> +-+-----------+-----------+-----------+----------------+-----------+--------+-----+----+------+ #> Best-so-far configuration:           3    mean value:     0.2421875000 #> Description of the best-so-far configuration: #>   .ID.                cp .PARENT. #> 3    3 -4.97378737969635       NA #>  #> # 2023-01-30 11:16:21 UTC: Elite configurations (first number is the configuration ID; listed from best to worst according to the sum of ranks): #>                  cp #> 3 -4.97378737969635 #> 2 -5.84268033068051 #> # 2023-01-30 11:16:21 UTC: Iteration 2 of 2 #> # experimentsUsedSoFar: 15 #> # remainingBudget: 27 #> # currentBudget: 27 #> # nbConfigurations: 5 #> # Markers: #>      x No test is performed. #>      c Configurations are discarded only due to capping. #>      - The test is performed and some configurations are discarded. #>      = The test is performed but no configuration is discarded. #>      ! The test is performed and configurations could be discarded but elite configurations are preserved. #>      . All alive configurations are elite and nothing is discarded #>  #> +-+-----------+-----------+-----------+----------------+-----------+--------+-----+----+------+ #> | |   Instance|      Alive|       Best|       Mean best| Exp so far|  W time|  rho|KenW|  Qvar| #> +-+-----------+-----------+-----------+----------------+-----------+--------+-----+----+------+ #> |x|          6|          5|          3|    0.2148437500|          5|00:00:00|   NA|  NA|    NA| #> |x|          4|          5|          3|    0.2285156250|          8|00:00:00|+0.34|0.67|0.5519| #> |x|          1|          5|          3|    0.2265625000|         11|00:00:00|+0.38|0.58|0.5142| #> |x|          3|          5|          5|    0.2392578125|         14|00:00:00|+0.17|0.38|0.7353| #> |=|          5|          5|          5|    0.2375000000|         17|00:00:00|+0.25|0.40|0.6966| #> |=|          2|          5|          3|    0.2376302083|         20|00:00:00|+0.25|0.37|0.6781| #> |=|          7|          5|          3|    0.2382812500|         25|00:00:00|+0.19|0.31|0.6821| #> +-+-----------+-----------+-----------+----------------+-----------+--------+-----+----+------+ #> Best-so-far configuration:           3    mean value:     0.2382812500 #> Description of the best-so-far configuration: #>   .ID.                cp .PARENT. #> 3    3 -4.97378737969635       NA #>  #> # 2023-01-30 11:16:22 UTC: Elite configurations (first number is the configuration ID; listed from best to worst according to the sum of ranks): #>                  cp #> 3 -4.97378737969635 #> 5 -4.74820552908674 #> # 2023-01-30 11:16:22 UTC: Stopped because there is not enough budget left to race more than the minimum (2) #> # You may either increase the budget or set 'minNbSurvival' to a lower value #> # Iteration: 3 #> # nbIterations: 3 #> # experimentsUsedSoFar: 40 #> # timeUsed: 0 #> # remainingBudget: 2 #> # currentBudget: 2 #> # number of elites: 2 #> # nbConfigurations: 2 #> # Total CPU user time: 2.314, CPU sys time: 0, Wall-clock time: 2.314  # best performing hyperparameter configuration instance$result #>           cp configuration learner_param_vals  x_domain classif.ce #> 1: -4.973787             3          <list[2]> <list[1]>  0.2382812  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -3.167937  0.2578125 0.0420903549            0.011 2023-01-30 11:16:20 #>  2: -5.842680  0.2265625 0.0029010564            0.011 2023-01-30 11:16:20 #>  3: -4.973787  0.2226562 0.0069169014            0.011 2023-01-30 11:16:20 #>  4: -3.167937  0.2578125 0.0420903549            0.011 2023-01-30 11:16:20 #>  5: -5.842680  0.2617188 0.0029010564            0.011 2023-01-30 11:16:20 #>  6: -4.973787  0.2460938 0.0069169014            0.033 2023-01-30 11:16:20 #>  7: -3.167937  0.2890625 0.0420903549            0.011 2023-01-30 11:16:21 #>  8: -5.842680  0.2695312 0.0029010564            0.011 2023-01-30 11:16:21 #>  9: -4.973787  0.2695312 0.0069169014            0.011 2023-01-30 11:16:21 #> 10: -3.167937  0.2656250 0.0420903549            0.011 2023-01-30 11:16:21 #> 11: -5.842680  0.2421875 0.0029010564            0.012 2023-01-30 11:16:21 #> 12: -4.973787  0.2421875 0.0069169014            0.012 2023-01-30 11:16:21 #> 13: -3.167937  0.3046875 0.0420903549            0.011 2023-01-30 11:16:21 #> 14: -5.842680  0.2421875 0.0029010564            0.012 2023-01-30 11:16:21 #> 15: -4.973787  0.2304688 0.0069169014            0.014 2023-01-30 11:16:21 #> 16: -4.973787  0.2148438 0.0069169014            0.012 2023-01-30 11:16:21 #> 17: -5.842680  0.2460938 0.0029010564            0.011 2023-01-30 11:16:21 #> 18: -4.205135  0.2148438 0.0149187779            0.011 2023-01-30 11:16:21 #> 19: -4.748206  0.2148438 0.0086672344            0.011 2023-01-30 11:16:21 #> 20: -8.158807  0.2617188 0.0002862035            0.011 2023-01-30 11:16:21 #> 21: -4.205135  0.2734375 0.0149187779            0.013 2023-01-30 11:16:21 #> 22: -4.748206  0.2578125 0.0086672344            0.011 2023-01-30 11:16:21 #> 23: -8.158807  0.2812500 0.0002862035            0.013 2023-01-30 11:16:21 #> 24: -4.205135  0.2304688 0.0149187779            0.012 2023-01-30 11:16:22 #> 25: -4.748206  0.2226562 0.0086672344            0.012 2023-01-30 11:16:22 #> 26: -8.158807  0.2265625 0.0002862035            0.011 2023-01-30 11:16:22 #> 27: -4.205135  0.2421875 0.0149187779            0.012 2023-01-30 11:16:22 #> 28: -4.748206  0.2617188 0.0086672344            0.012 2023-01-30 11:16:22 #> 29: -8.158807  0.2695312 0.0002862035            0.012 2023-01-30 11:16:22 #> 30: -4.205135  0.2617188 0.0149187779            0.011 2023-01-30 11:16:22 #> 31: -4.748206  0.2304688 0.0086672344            0.012 2023-01-30 11:16:22 #> 32: -8.158807  0.2421875 0.0002862035            0.011 2023-01-30 11:16:22 #> 33: -4.205135  0.2500000 0.0149187779            0.012 2023-01-30 11:16:22 #> 34: -4.748206  0.2578125 0.0086672344            0.012 2023-01-30 11:16:22 #> 35: -8.158807  0.2578125 0.0002862035            0.012 2023-01-30 11:16:22 #> 36: -4.973787  0.2421875 0.0069169014            0.013 2023-01-30 11:16:22 #> 37: -5.842680  0.2500000 0.0029010564            0.011 2023-01-30 11:16:22 #> 38: -4.205135  0.2578125 0.0149187779            0.011 2023-01-30 11:16:22 #> 39: -4.748206  0.2578125 0.0086672344            0.012 2023-01-30 11:16:22 #> 40: -8.158807  0.2500000 0.0002862035            0.011 2023-01-30 11:16:22 #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>     batch_nr race step instance configuration warnings errors #>  1:        1    1    1        7             1        0      0 #>  2:        1    1    1        7             2        0      0 #>  3:        1    1    1        7             3        0      0 #>  4:        2    1    1        3             1        0      0 #>  5:        2    1    1        3             2        0      0 #>  6:        2    1    1        3             3        0      0 #>  7:        3    1    1        4             1        0      0 #>  8:        3    1    1        4             2        0      0 #>  9:        3    1    1        4             3        0      0 #> 10:        4    1    1       10             1        0      0 #> 11:        4    1    1       10             2        0      0 #> 12:        4    1    1       10             3        0      0 #> 13:        5    1    1        8             1        0      0 #> 14:        5    1    1        8             2        0      0 #> 15:        5    1    1        8             3        0      0 #> 16:        6    2    1        1             3        0      0 #> 17:        6    2    1        1             2        0      0 #> 18:        6    2    1        1             4        0      0 #> 19:        6    2    1        1             5        0      0 #> 20:        6    2    1        1             6        0      0 #> 21:        7    2    1       10             4        0      0 #> 22:        7    2    1       10             5        0      0 #> 23:        7    2    1       10             6        0      0 #> 24:        8    2    1        7             4        0      0 #> 25:        8    2    1        7             5        0      0 #> 26:        8    2    1        7             6        0      0 #> 27:        9    2    1        4             4        0      0 #> 28:        9    2    1        4             5        0      0 #> 29:        9    2    1        4             6        0      0 #> 30:       10    2    1        8             4        0      0 #> 31:       10    2    1        8             5        0      0 #> 32:       10    2    1        8             6        0      0 #> 33:       11    2    1        3             4        0      0 #> 34:       11    2    1        3             5        0      0 #> 35:       11    2    1        3             6        0      0 #> 36:       12    2    1        2             3        0      0 #> 37:       12    2    1        2             2        0      0 #> 38:       12    2    1        2             4        0      0 #> 39:       12    2    1        2             5        0      0 #> 40:       12    2    1        2             6        0      0 #>     batch_nr race step instance configuration warnings errors #>          resample_result #>  1: <ResampleResult[21]> #>  2: <ResampleResult[21]> #>  3: <ResampleResult[21]> #>  4: <ResampleResult[21]> #>  5: <ResampleResult[21]> #>  6: <ResampleResult[21]> #>  7: <ResampleResult[21]> #>  8: <ResampleResult[21]> #>  9: <ResampleResult[21]> #> 10: <ResampleResult[21]> #> 11: <ResampleResult[21]> #> 12: <ResampleResult[21]> #> 13: <ResampleResult[21]> #> 14: <ResampleResult[21]> #> 15: <ResampleResult[21]> #> 16: <ResampleResult[21]> #> 17: <ResampleResult[21]> #> 18: <ResampleResult[21]> #> 19: <ResampleResult[21]> #> 20: <ResampleResult[21]> #> 21: <ResampleResult[21]> #> 22: <ResampleResult[21]> #> 23: <ResampleResult[21]> #> 24: <ResampleResult[21]> #> 25: <ResampleResult[21]> #> 26: <ResampleResult[21]> #> 27: <ResampleResult[21]> #> 28: <ResampleResult[21]> #> 29: <ResampleResult[21]> #> 30: <ResampleResult[21]> #> 31: <ResampleResult[21]> #> 32: <ResampleResult[21]> #> 33: <ResampleResult[21]> #> 34: <ResampleResult[21]> #> 35: <ResampleResult[21]> #> 36: <ResampleResult[21]> #> 37: <ResampleResult[21]> #> 38: <ResampleResult[21]> #> 39: <ResampleResult[21]> #> 40: <ResampleResult[21]> #>          resample_result  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task) # }"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"Subclass non-linear optimization (NLopt). Calls nloptr::nloptr package nloptr.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"Johnson, G S (2020). “NLopt nonlinear-optimization package.” https://github.com/stevengj/nlopt.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"termination conditions stopval, maxtime maxeval nloptr::nloptr() deactivated replaced bbotk::Terminator subclasses. x function value tolerance termination conditions (xtol_rel = 10^-4, xtol_abs = rep(0.0, length(x0)), ftol_rel = 0.0 ftol_abs = 0.0) still available implemented package defaults. deactivate conditions, set -1.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"nloptr\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"Tuner based bbotk::OptimizerNLoptr can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"algorithm character(1) eval_g_ineq function() xtol_rel numeric(1) xtol_abs numeric(1) ftol_rel numeric(1) ftol_abs numeric(1) start_values character(1) Create random start values based center search space? latter case, center parameters trafo applied. meaning control parameters, see nloptr::nloptr() nloptr::nloptr.print.options(). termination conditions stopval, maxtime maxeval nloptr::nloptr() deactivated replaced Terminator subclasses. x function value tolerance termination conditions (xtol_rel = 10^-4, xtol_abs = rep(0.0, length(x0)), ftol_rel = 0.0 ftol_abs = 0.0) still available implemented package defaults. deactivate conditions, set -1.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerNLoptr","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"TunerNLoptr$new() TunerNLoptr$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"","code":"TunerNLoptr$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"","code":"TunerNLoptr$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Non-linear Optimization — mlr_tuners_nloptr","text":"","code":"# Hyperparameter Optimization # \\donttest{  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # run hyperparameter tuning on the Palmer Penguins data set instance = tune(   method = tnr(\"nloptr\", algorithm = \"NLOPT_LN_BOBYQA\"),   task = tsk(\"penguins\"),   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\") )  # best performing hyperparameter configuration instance$result #>           cp learner_param_vals  x_domain classif.ce #> 1: -4.088389          <list[2]> <list[1]> 0.05217391  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce x_domain_cp runtime_learners           timestamp #>  1: -4.088389 0.05217391 0.016766224            0.008 2023-01-30 11:16:24 #>  2: -4.088389 0.05217391 0.016766224            0.007 2023-01-30 11:16:24 #>  3: -4.088389 0.05217391 0.016766224            0.008 2023-01-30 11:16:24 #>  4: -2.361450 0.05217391 0.094283407            0.007 2023-01-30 11:16:24 #>  5: -5.815328 0.05217391 0.002981503            0.008 2023-01-30 11:16:24 #>  6: -4.071119 0.05217391 0.017058281            0.008 2023-01-30 11:16:24 #>  7: -4.105658 0.05217391 0.016479168            0.007 2023-01-30 11:16:24 #>  8: -4.086662 0.05217391 0.016795203            0.008 2023-01-30 11:16:24 #>  9: -4.090116 0.05217391 0.016737295            0.007 2023-01-30 11:16:24 #> 10: -4.088389 0.05217391 0.016766224            0.008 2023-01-30 11:16:24 #>     batch_nr warnings errors      resample_result #>  1:        1        0      0 <ResampleResult[21]> #>  2:        2        0      0 <ResampleResult[21]> #>  3:        3        0      0 <ResampleResult[21]> #>  4:        4        0      0 <ResampleResult[21]> #>  5:        5        0      0 <ResampleResult[21]> #>  6:        6        0      0 <ResampleResult[21]> #>  7:        7        0      0 <ResampleResult[21]> #>  8:        8        0      0 <ResampleResult[21]> #>  9:        9        0      0 <ResampleResult[21]> #> 10:       10        0      0 <ResampleResult[21]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"penguins\")) # }"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"Subclass random search tuning.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"Bergstra J, Bengio Y (2012). “Random Search Hyper-Parameter Optimization.” Journal Machine Learning Research, 13(10), 281--305. https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"random points sampled paradox::generate_design_random().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"Tuner can instantiated associated sugar function tnr():","code":"tnr(\"random_search\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"optimizer","dir":"Reference","previous_headings":"","what":"Optimizer","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"Tuner based bbotk::OptimizerRandomSearch can applied black box optimization problem. See also documentation bbotk.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"batch_size integer(1) Maximum number points try batch.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"book section tuners. mlr3hyperband extension package Hyperband algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerRandomSearch","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$help() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"TunerRandomSearch$new() TunerRandomSearch$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"","code":"TunerRandomSearch$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"","code":"TunerRandomSearch$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Random Search — mlr_tuners_random_search","text":"","code":"# Hyperparameter Optimization  # load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # run hyperparameter tuning on the Palmer Penguins data set instance = tune(   method = \"random_search\",   task = tsk(\"penguins\"),   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10 )  # best performing hyperparameter configuration instance$result #>           cp learner_param_vals  x_domain classif.ce #> 1: -8.736647          <list[2]> <list[1]> 0.02608696  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -4.325632 0.04347826 0.0132251887            0.008 2023-01-30 11:16:25 #>  2: -2.485935 0.04347826 0.0832476950            0.007 2023-01-30 11:16:25 #>  3: -4.679870 0.04347826 0.0092802172            0.007 2023-01-30 11:16:25 #>  4: -8.736647 0.02608696 0.0001605915            0.008 2023-01-30 11:16:25 #>  5: -4.345245 0.04347826 0.0129683249            0.008 2023-01-30 11:16:25 #>  6: -6.908688 0.02608696 0.0009990674            0.007 2023-01-30 11:16:26 #>  7: -4.334599 0.04347826 0.0131071317            0.007 2023-01-30 11:16:26 #>  8: -5.475992 0.02608696 0.0041860760            0.007 2023-01-30 11:16:26 #>  9: -4.998290 0.02608696 0.0067494756            0.008 2023-01-30 11:16:26 #> 10: -3.318467 0.04347826 0.0362082936            0.007 2023-01-30 11:16:26 #>     batch_nr warnings errors      resample_result #>  1:        1        0      0 <ResampleResult[21]> #>  2:        2        0      0 <ResampleResult[21]> #>  3:        3        0      0 <ResampleResult[21]> #>  4:        4        0      0 <ResampleResult[21]> #>  5:        5        0      0 <ResampleResult[21]> #>  6:        6        0      0 <ResampleResult[21]> #>  7:        7        0      0 <ResampleResult[21]> #>  8:        8        0      0 <ResampleResult[21]> #>  9:        9        0      0 <ResampleResult[21]> #> 10:       10        0      0 <ResampleResult[21]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(tsk(\"penguins\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. bbotk mlr_terminators, trm, trms mlr3misc clbk, clbks, mlr_callbacks","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ti.html","id":null,"dir":"Reference","previous_headings":"","what":"Syntactic Sugar for Tuning Instance Construction — ti","title":"Syntactic Sugar for Tuning Instance Construction — ti","text":"Function construct TuningInstanceSingleCrit TuningInstanceMultiCrit.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ti.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Syntactic Sugar for Tuning Instance Construction — ti","text":"","code":"ti(   task,   learner,   resampling,   measures = NULL,   terminator,   search_space = NULL,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   allow_hotstart = FALSE,   keep_hotstart_stack = FALSE,   evaluate_default = FALSE,   callbacks = list() )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ti.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Syntactic Sugar for Tuning Instance Construction — ti","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (mlr3::Measure list mlr3::Measure) single measure creates TuningInstanceSingleCrit multiple measures TuningInstanceMultiCrit. NULL, default measure used. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. keep_hotstart_stack (logical(1)) TRUE, mlr3::HotstartStack kept $objective$hotstart_stack tuning. evaluate_default (logical(1)) TRUE, learner evaluated hyperparameters set default values start optimization. callbacks (list CallbackTuning) List callbacks.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ti.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Syntactic Sugar for Tuning Instance Construction — ti","text":"book chapter hyperparameter optimization. book chapter tuning spaces. gallery post tuning svm. mlr3tuningspaces extension package.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ti.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Syntactic Sugar for Tuning Instance Construction — ti","text":"","code":"# Hyperparameter optimization on the Palmer Penguins data set task = tsk(\"penguins\")  # Load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # Construct tuning instance instance = ti(   task = task,   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measures = msr(\"classif.ce\"),   terminator = trm(\"evals\", n_evals = 4) )  # Choose optimization algorithm tuner = tnr(\"random_search\", batch_size = 2)  # Run tuning tuner$optimize(instance) #>           cp learner_param_vals  x_domain classif.ce #> 1: -7.581894          <list[2]> <list[1]> 0.05809814  # Set optimal hyperparameter configuration to learner learner$param_set$values = instance$result_learner_param_vals  # Train the learner on the full data set learner$train(task)  # Inspect all evaluated configurations as.data.table(instance$archive) #>           cp classif.ce  x_domain_cp runtime_learners           timestamp #> 1: -3.584188 0.06099669 0.0277592081            0.021 2023-01-30 11:16:27 #> 2: -2.930252 0.06099669 0.0533836088            0.021 2023-01-30 11:16:27 #> 3: -7.581894 0.05809814 0.0005095954            0.022 2023-01-30 11:16:27 #> 4: -8.808853 0.05809814 0.0001494046            0.021 2023-01-30 11:16:27 #>    batch_nr warnings errors      resample_result #> 1:        1        0      0 <ResampleResult[21]> #> 2:        1        0      0 <ResampleResult[21]> #> 3:        2        0      0 <ResampleResult[21]> #> 4:        2        0      0 <ResampleResult[21]>"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":null,"dir":"Reference","previous_headings":"","what":"Syntactic Sugar for Tuning Objects Construction — tnr","title":"Syntactic Sugar for Tuning Objects Construction — tnr","text":"Functions retrieve objects, set parameters assign fields one go. Relies mlr3misc::dictionary_sugar_get() extract objects respective mlr3misc::Dictionary: tnr() Tuner mlr_tuners. tnrs() list Tuners mlr_tuners. trm() Terminator mlr_terminators. trms() list Terminators mlr_terminators.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Syntactic Sugar for Tuning Objects Construction — tnr","text":"","code":"tnr(.key, ...)  tnrs(.keys, ...)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Syntactic Sugar for Tuning Objects Construction — tnr","text":".key (character(1)) Key passed respective dictionary retrieve object. ... (named list()) Named arguments passed constructor, set parameters paradox::ParamSet, set public field. See mlr3misc::dictionary_sugar_get() details. .keys (character()) Keys passed respective dictionary retrieve multiple objects.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Syntactic Sugar for Tuning Objects Construction — tnr","text":"R6::R6Class object respective type, list R6::R6Class objects plural versions.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Syntactic Sugar for Tuning Objects Construction — tnr","text":"","code":"# random search tuner with batch size of 5 tnr(\"random_search\", batch_size = 5) #> <TunerRandomSearch>: Random Search #> * Parameters: batch_size=5 #> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct #> * Properties: dependencies, single-crit, multi-crit #> * Packages: mlr3tuning, bbotk  # run time terminator with 20 seconds trm(\"run_time\", secs = 20) #> <TerminatorRunTime>: Run Time #> * Parameters: secs=20"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for Tuning a Learner — tune","title":"Function for Tuning a Learner — tune","text":"Function tune mlr3::Learner. function internally creates TuningInstanceSingleCrit TuningInstanceMultiCrit describe tuning problem. executes tuning Tuner (method) returns result tuning instance ($result). ArchiveTuning ($archive) stores evaluated hyperparameter configurations performance scores.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for Tuning a Learner — tune","text":"","code":"tune(   method,   task,   learner,   resampling,   measures = NULL,   term_evals = NULL,   term_time = NULL,   terminator = NULL,   search_space = NULL,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   allow_hotstart = FALSE,   keep_hotstart_stack = FALSE,   evaluate_default = FALSE,   callbacks = list(),   ... )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for Tuning a Learner — tune","text":"method (character(1) | Tuner) Key retrieve tuner mlr_tuners dictionary Tuner object. task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluate performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (mlr3::Measure list mlr3::Measure) single measure creates TuningInstanceSingleCrit multiple measures TuningInstanceMultiCrit. NULL, default measure used. term_evals (integer(1)) Number allowed evaluations. term_time (integer(1)) Maximum allowed time seconds. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. keep_hotstart_stack (logical(1)) TRUE, mlr3::HotstartStack kept $objective$hotstart_stack tuning. evaluate_default (logical(1)) TRUE, learner evaluated hyperparameters set default values start optimization. callbacks (list CallbackTuning) List callbacks. ... (named list()) Named arguments set parameters tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for Tuning a Learner — tune","text":"TuningInstanceSingleCrit | TuningInstanceMultiCrit","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Function for Tuning a Learner — tune","text":"mlr3::Task, mlr3::Learner, mlr3::Resampling, mlr3::Measure Terminator used construct TuningInstanceSingleCrit. multiple performance Measures supplied, TuningInstanceMultiCrit created. parameter term_evals term_time shortcuts create Terminator. parameters passed, TerminatorCombo constructed. Terminators, pass one terminator. termination criterion needed, set term_evals, term_time terminator NULL. search space created paradox::TuneToken supplied search_space.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"resources","dir":"Reference","previous_headings":"","what":"Resources","title":"Function for Tuning a Learner — tune","text":"book chapter hyperparameter optimization. book chapter tuning spaces. gallery post tuning svm. mlr3tuningspaces extension package.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"analysis","dir":"Reference","previous_headings":"","what":"Analysis","title":"Function for Tuning a Learner — tune","text":"analyzing tuning results, recommended pass ArchiveTuning .data.table(). returned data table joined benchmark result adds mlr3::ResampleResult hyperparameter evaluation. archive provides various getters (e.g. $learners()) ease access. getters extract position () unique hash (uhash). complete list getters see methods section. benchmark result ($benchmark_result) allows score hyperparameter configurations different measure. Alternatively, measures can supplied .data.table(). mlr3viz package provides visualizations tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for Tuning a Learner — tune","text":"","code":"# Hyperparameter optimization on the Palmer Penguins data set task = tsk(\"pima\")  # Load learner and set search space learner = lrn(\"classif.rpart\",   cp = to_tune(1e-04, 1e-1, logscale = TRUE) )  # Run tuning instance = tune(   method = tnr(\"random_search\", batch_size = 2),   task = tsk(\"pima\"),   learner = learner,   resampling = rsmp (\"holdout\"),   measures = msr(\"classif.ce\"),   terminator = trm(\"evals\", n_evals = 4) )  # Set optimal hyperparameter configuration to learner learner$param_set$values = instance$result_learner_param_vals  # Train the learner on the full data set learner$train(task)  # Inspect all evaluated configurations as.data.table(instance$archive) #>           cp classif.ce  x_domain_cp runtime_learners           timestamp #> 1: -8.594633  0.2500000 0.0001850966            0.012 2023-01-30 11:16:28 #> 2: -6.642437  0.2500000 0.0013038458            0.011 2023-01-30 11:16:28 #> 3: -3.092784  0.2734375 0.0453754362            0.011 2023-01-30 11:16:28 #> 4: -4.169738  0.2656250 0.0154563015            0.012 2023-01-30 11:16:28 #>    batch_nr warnings errors      resample_result #> 1:        1        0      0 <ResampleResult[21]> #> 2:        1        0      0 <ResampleResult[21]> #> 3:        2        0      0 <ResampleResult[21]> #> 4:        2        0      0 <ResampleResult[21]>"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for Nested Resampling — tune_nested","title":"Function for Nested Resampling — tune_nested","text":"Function conduct nested resampling.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for Nested Resampling — tune_nested","text":"","code":"tune_nested(   method,   task,   learner,   inner_resampling,   outer_resampling,   measure = NULL,   term_evals = NULL,   term_time = NULL,   search_space = NULL,   ... )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for Nested Resampling — tune_nested","text":"method (character(1)) Key retrieve tuner mlr_tuners dictionary. task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. inner_resampling (mlr3::Resampling) Resampling used inner loop. outer_resampling mlr3::Resampling) Resampling used outer loop. measure (mlr3::Measure) Measure optimize. NULL, default measure used. term_evals (integer(1)) Number allowed evaluations. term_time (integer(1)) Maximum allowed time seconds. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). ... (named list()) Named arguments set parameters tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for Nested Resampling — tune_nested","text":"mlr3::ResampleResult","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for Nested Resampling — tune_nested","text":"","code":"# Nested resampling on Palmer Penguins data set rr = tune_nested(   method = \"random_search\",   task = tsk(\"penguins\"),   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   inner_resampling = rsmp (\"holdout\"),   outer_resampling = rsmp(\"cv\", folds = 2),   measure = msr(\"classif.ce\"),   term_evals = 2,   batch_size = 2)  # Performance scores estimated on the outer resampling rr$score() #>                 task  task_id         learner          learner_id #> 1: <TaskClassif[50]> penguins <AutoTuner[46]> classif.rpart.tuned #> 2: <TaskClassif[50]> penguins <AutoTuner[46]> classif.rpart.tuned #>            resampling resampling_id iteration              prediction #> 1: <ResamplingCV[20]>            cv         1 <PredictionClassif[20]> #> 2: <ResamplingCV[20]>            cv         2 <PredictionClassif[20]> #>    classif.ce #> 1: 0.02906977 #> 2: 0.06395349  # Unbiased performance of the final model trained on the full data set rr$aggregate() #> classif.ce  #> 0.04651163"},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0172","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.17.2","title":"mlr3tuning 0.17.2","text":"CRAN release: 2022-12-22 feat: AutoTuner accepts instantiated resamplings now. AutoTuner checks row ids inner resampling present outer resampling train set nested resampling performed. fix: Standalone Tuner create ContextOptimization.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0171","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.17.1","title":"mlr3tuning 0.17.1","text":"CRAN release: 2022-12-07 fix: ti() function accept callbacks.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0170","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.17.0","title":"mlr3tuning 0.17.0","text":"CRAN release: 2022-11-18 feat: methods $importance(), $selected_features(), $oob_error() $loglik() forwarded final model AutoTuner now. refactor: AutoTuner stores instance benchmark result store_models = TRUE. refactor: AutoTuner stores instance store_benchmark_result = TRUE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0160","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.16.0","title":"mlr3tuning 0.16.0","text":"CRAN release: 2022-11-08 feat: Add new callback enables early stopping tuning mlr_callbacks. feat: Add new callback backups benchmark result disk batch. feat: Create custom callbacks callback_tuning() function.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0150","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.15.0","title":"mlr3tuning 0.15.0","text":"CRAN release: 2022-10-21 fix: AutoTuner accept TuningSpace objects search spaces. feat: Add ti() function create TuningInstanceSingleCrit TuningInstanceMultiCrit. docs: Documentation technical details section now. feat: New option extract_inner_tuning_results() return tuning instances.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0140","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.14.0","title":"mlr3tuning 0.14.0","text":"CRAN release: 2022-08-25 feat: Add option evaluate_default evaluate learners hyperparameters set default values. refactor: now , default smooth FALSE TunerGenSA.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0131","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.13.1","title":"mlr3tuning 0.13.1","text":"CRAN release: 2022-05-03 feat: Tuner objects field $id now.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0130","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.13.0","title":"mlr3tuning 0.13.0","text":"CRAN release: 2022-04-06 feat: Allow pass Tuner objects method tune() auto_tuner(). docs: Link Tuner help page bbotk::Optimizer. feat: Tuner objects optional field $label now. feat: .data.table() functions objects class Dictionary extended additional columns.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0121","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.12.1","title":"mlr3tuning 0.12.1","text":"CRAN release: 2022-02-25 feat: Add .data.table.DictionaryTuner function. feat: New $help() method opens manual page Tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0120","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.12.0","title":"mlr3tuning 0.12.0","text":"CRAN release: 2022-02-17 feat: as_search_space() function create search spaces Learner ParamSet objects. Allow pass TuningSpace objects search_space TuningInstanceSingleCrit TuningInstanceMultiCrit. feat: mlr3::HotstartStack can now removed tuning keep_hotstart_stack flag. feat: Archive stores errors warnings learners. feat: measure provided, default measure used auto_tuner() tune_nested().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0110","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.11.0","title":"mlr3tuning 0.11.0","text":"CRAN release: 2022-02-02 fix: $assign_result() method TuningInstanceSingleCrit search space empty. feat: Default measure used measure supplied TuningInstanceSingleCrit.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0100","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.10.0","title":"mlr3tuning 0.10.0","text":"CRAN release: 2022-01-20 Fixes bug TuningInstanceMultiCrit$assign_result(). Hotstarting learners previously fitted models. Remove deep clones speed tuning. Add store_models flag auto_tuner(). Add \"noisy\" property ObjectiveTuning.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-090","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.9.0","title":"mlr3tuning 0.9.0","text":"CRAN release: 2021-09-14 Adds AutoTuner$base_learner() method extract base learner nested learner objects. tune() supports multi-criteria tuning. Allows empty search space. Adds TunerIrace irace package. extract_inner_tuning_archives() helper function extract inner tuning archives. Removes ArchiveTuning$extended_archive() method. mlr3::ResampleResults joined automatically .data.table.TuningArchive() extract_inner_tuning_archives().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-080","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.8.0","title":"mlr3tuning 0.8.0","text":"CRAN release: 2021-03-12 Adds tune(), auto_tuner() tune_nested() sugar functions. TuningInstanceSingleCrit, TuningInstanceMultiCrit AutoTuner can initialized store_benchmark_result = FALSE store_models = TRUE allow measures access models. Prettier printing methods.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-070","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.7.0","title":"mlr3tuning 0.7.0","text":"CRAN release: 2021-02-11 Fix TuningInstance*$assign_result() errors required parameter bug. Shortcuts access $learner(), $learners(), $learner_param_vals(), $predictions() $resample_result() benchmark result archive. extract_inner_tuning_results() helper function extract inner tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-060","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.6.0","title":"mlr3tuning 0.6.0","text":"CRAN release: 2021-01-24 ArchiveTuning$data public field now.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-050","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.5.0","title":"mlr3tuning 0.5.0","text":"CRAN release: 2020-12-07 Adds TunerCmaes adagio package. Fix predict_type AutoTuner. Support set TuneToken Learner$param_set create search space . order parameters TuningInstanceSingleCrit TuningInstanceSingleCrit changed.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-040","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.4.0","title":"mlr3tuning 0.4.0","text":"CRAN release: 2020-10-07 Option control store_benchmark_result, store_models check_values AutoTuner. store_tuning_instance must set parameter initialization. Fixes check_values flag TuningInstanceSingleCrit TuningInstanceMultiCrit. Removed dependency orphaned package bibtex.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-030","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.3.0","title":"mlr3tuning 0.3.0","text":"CRAN release: 2020-09-08 Compact -memory representation R6 objects save space saving mlr3 objects via saveRDS(), serialize() etc. Archive ArchiveTuning now stores benchmark result $benchmark_result. change removed resample results archive can still accessed via benchmark result. Warning message external package tuning installed. retrieve inner tuning results nested resampling, .data.table(rr)$learner[[1]]$tuning_result must used now.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-020","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.2.0","title":"mlr3tuning 0.2.0","text":"CRAN release: 2020-07-28 TuningInstance now TuningInstanceSingleCrit. TuningInstanceMultiCrit still available multi-criteria tuning. Terminators now accessible trm() trms() instead term() terms(). Storing resample results optional now using store_resample_result flag TuningInstanceSingleCrit TuningInstanceMultiCrit TunerNLoptr adds non-linear optimization nloptr package. Logging controlled bbotk logger now. Proposed points performance values can checked validity activating check_values flag TuningInstanceSingleCrit TuningInstanceMultiCrit.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-013","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.3","title":"mlr3tuning 0.1.3","text":"mlr3tuning now depends bbotk package basic tuning objects. Terminator classes now live bbotk. consequence ObjectiveTuning inherits bbotk::Objective, TuningInstance bbotk::OptimInstance Tuner bbotk::Optimizer TuningInstance$param_set becomes TuningInstance$search_space avoid confusion param_set usually contains parameters change behavior object. Tuning triggered $optimize() instead $tune()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-012","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.2","title":"mlr3tuning 0.1.2","text":"CRAN release: 2020-01-31 Fixed bug AutoTuner $clone() missing. Tuning results unaffected, stored models contained wrong hyperparameter values (#223). Improved output log (#218).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-011","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.1","title":"mlr3tuning 0.1.1","text":"CRAN release: 2019-12-06 Maintenance release.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-010","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.0","title":"mlr3tuning 0.1.0","text":"CRAN release: 2019-09-30 Initial prototype.","code":""}]
